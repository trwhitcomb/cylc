
\lstset{language=transcript}

\section{Introduction: How Cylc Works} 
\label{HowCylcWorks}

\subsection{Scheduling Forecast Suites} 
\label{SchedulingForecastSuites}

Environmental forecasting suites generate forecast products from a
potentially large group of interdependent scientific models and
associated data processing tasks. They are constrained by availability
of external driving data: typically one or more tasks will wait on real
time observations and/or model data from an external system, and these
will drive other downstream tasks, and so on. The dependency diagram for
a single forecast cycle in such a system is a {\em Directed Acyclic
Graph} as shown in Figure~\ref{fig-dep-one} (in our terminology, a {\em
forecast cycle} is comprised of all tasks with a common {\em cycle
time}, which is the nominal analysis time or start time of the forecast
models in the group). In real time operation processing will consist of
a series of distinct forecast cycles that are each initiated, after a
gap, by arrival of the new cycle's external driving data.

From a job scheduling perspective task execution order in such a system
must be carefully controlled in order to avoid dependency violations.
Ideally, each task should be queued for execution at the instant its
last prerequisite is satisfied; this is the best that can be done even
if queued tasks are not able to execute immediately because of resource
contention.

\subsection{EcoConnect} 
\label{EcoConnect}

Cylc was developed for the EcoConnect Forecasting System at NIWA
(National Institute of Water and Atmospheric Research, New Zealand).
EcoConnect takes real time atmospheric and stream flow observations, and
operational global weather forecasts from the Met Office (UK), and uses
these to drive global sea state and regional data assimilating weather
models, which in turn drive regional sea state, storm surge, and
catchment river models, plus tide prediction, and a large number of
associated data collection, quality control, preprocessing,
postprocessing, product generation, and archiving tasks.\footnote{Future
plans for EcoConnect include additional deterministic regional weather
forecasts and a statistical ensemble.} The global sea state forecast
runs once daily. The regional weather forecast runs four times daily but
it supplies surface winds and pressure to several downstream models that
run only twice daily, and precipitation accumulations to catchment river
models that run on an hourly cycle assimilating real time stream flow
observations and using the most recently available regional weather
forecast.  EcoConnect runs on heterogenous distributed hardware,
including a massively parallel supercomputer and several Linux servers. 


\subsection{Dependence Between Tasks}

\subsubsection{Intracycle Dependence} 
\label{IntracycleDependence}

Most inter-task dependence exist within a single forecast cycle.
Figure~\ref{fig-dep-one} shows the dependency diagram for a single
forecast cycle of a simple example suite of three forecast models ({\em
a, b,} and {\em c}) and three post processing or product generation
tasks ({\em d, e} and {\em f}). A scheduler capable of handling this
must manage, within a single forecast cycle, multiple parallel streams
of execution that branch when one task generates output for several
downstream tasks, and merge when one task takes input from several
upstream tasks. 

\begin{figure}
    \begin{center}
        \includegraphics[width=6cm]{images/dep-one-cycle.png} 
    \end{center}
    \caption[A single cycle dependency graph for a simple suite]{\scriptsize
    The dependency graph for a single forecast cycle of a simple example
    suite. Tasks {\em a, b,} and {\em c} represent forecast models,
    {\em d, e} and {\em f} are post processing or product generation
    tasks, and {\em x} represents external data that the upstream
    forecast model depends on.}
    \label{fig-dep-one} 
\end{figure} 

\begin{figure}
    \begin{center}
        \includegraphics[width=8cm]{images/timeline-one.png}
    \end{center}
    \caption[A single cycle job schedule for real time operation]{\scriptsize
    The optimal job schedule for two consecutive cycles of our example
    suite during real time operation, assuming that all tasks trigger 
    off upstream tasks finishing completely. The horizontal extent of
    a task bar represents its execution time, and the vertical blue
    lines show when the external driving data becomes available.}
    \label{fig-time-one}
\end{figure}

Figure~\ref{fig-time-one} shows the optimal job schedule for two
consecutive cycles of the example suite in real time operation, given
execution times represented by the horizontal extent of the task bars.
There is a time gap between cycles as the suite waits on new external
driving data.  Each task in the example suite happens to trigger off
upstream tasks {\em finishing}, rather than off any intermediate output
or event; this is merely a simplification that makes for clearer
diagrams.

\begin{figure}
    \begin{center}
        \includegraphics[width=10cm]{images/dep-two-cycles-linked.png} 
    \end{center}
    \caption[What if the external driving data is available early?]{\scriptsize If
    the external driving data is available in advance, can we start
    running the next cycle early?} 
    \label{fig-dep-two-linked}
\end{figure}

\begin{figure}
    \begin{center}
        \includegraphics[width=6cm]{images/timeline-one-c.png} 
    \end{center}
    \caption[Attempted overlap of consective single-cycle job
    schedules]{\scriptsize A naive attempt to overlap two consecutive cycles
    using the single-cycle dependency graph. The red shaded tasks will
    fail because of dependency violations (or will not be able to run
    because of upstream dependency violations).} 
    \label{fig-overlap}
\end{figure} 

\begin{figure}
    \begin{center}
        \includegraphics[width=8cm]{images/timeline-one-a.png} 
    \end{center}
    \caption[The only safe multicycle job schedule?]{\scriptsize The best
    that can be done {\em in general} when intercycle dependence is
    ignored.} 
    \label{fig-job-no-overlap}
\end{figure} 

Now the question arises, what happens if the external driving data for
upcoming cycles is available in advance, as it would be after a
significant delay in operations, or when running a historical case
study?  While the forecast model {\em a} appears to depend only on the
external data {\em x} at this stage of the discussion, in fact it would 
typically also depend on its own previous instance for the model {\em
background state} used in initializing the new forecast. Thus, as
alluded to in Figure~\ref{fig-dep-two-linked}, task {\em a} could in
principle start
as soon as its predecessor has finished.  Figure~\ref{fig-overlap}
shows, however, that starting a whole new cycle at this point is
dangerous - it results in dependency violations in half of the tasks in
the example suite. In fact the situation is even worse than this
- imagine that task {\em b} in the first cycle is delayed for any reason
{\em after} the second cycle has been launched? Clearly we must consider
handling intercycle dependence explicitly or else agree not to start
the next cycle early, as is illustrated in Figure~\ref{fig-job-no-overlap}.

\subsubsection{Intercycle Dependence} 
\label{IntercycleDependence}

In most suites dependence between tasks in different cycles
exist. Forecast models, as above, typically depend on their own most
recent previous forecast for a background state, and different
types of tasks in different forecast cycles can also be linked (in an 
atmospheric forecast analysis suite, for instance, the weather model 
may also generate background states for use by the observation
processing and data-assimilation systems in the next cycle). In real
time operation this intercycle dependence
can be ignored because it is automatically satisfied when each cycle
finishes before the next one begins. This is just as well
because they dramatically increase the complexity of the dependency
graph of even the simplest suites, by destroying the clean boundary
between forecast cycles. Figure~\ref{fig-dep-multi} illustrates the
problem for our simple example suite assuming the minimal likely
intercycle dependence: the forecast models ($a$, $b$, and $c$) each
depend on their own previous instances.

For this reason, and because we tend to imagine that forecasting suites
always run in distinct cycles, existing metaschedulers (as far as the author
is aware!) ignore intercycle dependence and therefore {\em require} a
series of distinct cycles at all times. While this does not affect
normal real time operation it can be a serious impediment when advance
availability of external driving data makes it possible, in principle,
to run some tasks from upcoming cycles before the current cycle is
finished - as suggested at the end of the previous section. This occurs
after delays (late arrival of external data, system maintenance, etc.)
and, to an even greater extent, in historical case studies, and parallel
test suites that are delayed with respect to the main operation. It is
a serious problem, in particular, for suites that have little downtime
between forecast cycles and therefore take many cycles to catch up
after a delay. Without taking account of intercycle dependence, the
best that can be done, in general, is to reduce the gap between cycles
to zero as shown in Figure~\ref{fig-job-no-overlap}. A limited crude
overlap of the single cycle job schedule may be possible for specific
task sets but the allowable overlap may change if new tasks are added,
and it is still dangerous: it amounts to running different parts of a
dependent system as if they were not dependent and as such it cannot be
guaranteed that some unforeseen delay in one cycle, after the 
next cycle has begun, (e.g.\ due to resource contention or task
failures) won't result in dependency violations.

\begin{figure}
    \begin{center}
        \includegraphics[width=8cm]{images/dep-multi-cycle.png} 
    \end{center}
    \caption[The complete multicycle dependency graph]{\scriptsize The complete
    dependency graph for the example suite, assuming the least possible
    intercycle dependence: the forecast models ($a$, $b$, and $c$)
    depend on their own previous instances. The dashed arrows show
    connections to previous and subsequent forecast cycles.} 
    \label{fig-dep-multi}
\end{figure}

\begin{figure}
    \begin{center}
        \includegraphics[width=6cm]{images/timeline-two-cycles-optimal.png} 
    \end{center}
    \caption[The optimal two-cycle job schedule]{\scriptsize The optimal two
    cycle job schedule when the next cycle's driving data is available in
    advance, possible in principle when intercycle dependence is
    handled explicitly.} 
    \label{fig-optimal-two}
\end{figure} 

Figure~\ref{fig-optimal-two} shows, in contrast to
Figure~\ref{fig-overlap}, the optimal two cycle job schedule obtained by
respecting all intercycle dependence.  This assumes no delays due to
resource contention or otherwise - i.e.\ every task runs
as soon as it is ready to run. The scheduler running
this suite must be able to adapt dynamically to external conditions 
that impact on multicycle scheduling in the presence of
intercycle dependence or else, again, risk bringing the system down
with dependency violations.

\begin{figure}
    \begin{center}
        \includegraphics[width=12cm]{images/timeline-three.png} 
    \end{center}
    \caption[Comparison of job schedules after a delay]{\scriptsize Job
    schedules for the example suite after a delay of almost one whole
    forecast cycle, when intercycle dependence is
    taken into account (above the time axis), and when it is not
    (below the time axis). The colored lines indicate the time that
    each cycle is delayed, and normal ``caught up'' cycles
    are shaded gray.} 
    \label{fig-time-three}
\end{figure} 

\begin{figure}
    \begin{center} 
        \includegraphics[width=8cm]{images/timeline-two.png}
    \end{center} 
    \caption[Optimal job schedule when all external data is
    available]{\scriptsize Job schedules for the example suite in case study
    mode, or after a long delay, when the external driving data are
    available many cycles in advance. Above the time axis is the optimal
    schedule obtained when the suite is constrained only by its true
    dependencies, as in Figure \ref{fig-dep-two-linked}, and underneath
    is the best that can be done, in general, when intercycle
    dependence is ignored.} 
    \label{fig-time-two}
\end{figure} 

To further illustrate the potential benefits of proper intercycle
dependency handling, Figure~\ref{fig-time-three} shows an operational
delay of almost one whole cycle in a suite with little downtime between
cycles. Above the time axis is the optimal schedule that is possible, in
principle, when intercycle dependence is taken into account, and
below is the only safe schedule possible {\em in general} when they are
ignored.  In the former case, even the cycle immediately after the delay
is hardly affected, and subsequent cycles are all on time, whilst in the
latter case it takes five full cycles to catch up to normal real time
operation.
%Note that simply overlapping the single cycle schedules of
%Figure~\ref{fig-time-one} from the same start point would have resulted
%in dependency violation by task {\em c}.

Similarly, Figure~\ref{fig-time-two} shows example suite job schedules
for an historical case study, or when catching up after a very long
delay; i.e.\ when the external driving data are available many cycles in
advance.  Task {\em a}, which as the most upstream forecast model is
likely to be a resource intensive atmosphere or ocean model, has no
upstream dependence on cotemporal tasks and can therefore run
continuously, regardless of how much downstream processing is yet to be
completed in its own, or any previous, forecast cycle (actually, task
{\em a} does depend on cotemporal task {\em x} which waits on the
external driving data, but that returns immediately when the external
data is available in advance, so the result stands). The other forecast
models can also cycle continuously or with short gap between, and some
post processing tasks, which have no previous-instance dependence, can
run continuously or even overlap (e.g.\ {\em e} in this case). Thus,
even for this very simple example suite, tasks from three or four
different cycles can in principle run simultaneously at any given time. 
In fact, if our tasks are able to trigger off internal outputs of 
upstream tasks, rather than waiting on full completion, successive
instances of the forecast models could overlap as well (because model
restart outputs are generally completed early in the forecast) for an
even more efficient job schedule. 

%Finally, we note again that a good job scheduler should be able to
%dynamically adapt to delays in any part of the suite due to resource
%contention, varying run times, or anything else that will inevitably
%modify the depicted job schedules. 

\subsection{The Cylc Scheduling Algorithm} 
\label{TheCylcSchedulingAlgorithm}

\begin{figure}
    \begin{center} 
        \includegraphics[width=8cm]{images/task-pool.png}
    \end{center} 
    \caption[The cylc task pool]{\scriptsize How cylc sees a suite, in
    contrast to the multicycle dependency graph of Figure~\ref{fig-dep-multi}.
    Task colors represent different cycle times, and the small squares
    and circles represent different prerequisites and outputs. A task
    can run when its prerequisites are satisfied by the outputs 
    of other tasks in the pool.} 
    \label{fig-task-pool}
\end{figure} 

Cylc manages a pool of proxy objects that represent real tasks in the
forecasting suite. A task proxy can run the real task that it
represents when its prerequisites are satisfied, and can receive reports
of completed outputs from the real task as it runs. There is no global
cycling mechanism to advance the suite in time; instead each individual
task proxy has a private cycle time and spawns its own successor. Task
proxies are self-contained - they just know their own prerequisites and
outputs and are not aware of the wider suite context. Intercycle
dependencies are not treated as special, and the task pool can be
populated with tasks from many different cycle times. 
The cylc task pool is illustrated in Figure~\ref{fig-task-pool}. Now,
{\em whenever any task changes state
due to completion of an output, every task checks to see if its
own prerequisites are now satisfied.}\footnote{In fact this dependency
negotiation goes through a broker object (rather than every task
literally checking every other task) which scales as $n$ (rather than
$n^2$) where $n$ is the number of task proxies in the pool.}  Moreover,
this matching of prerequisites and outputs involves the entire task
pool, regardless of individual cycle times, so that inter- and
intra-cycle dependence is handled with ease.

Thus without using global cycling mechanisms, and treating all
inter-task dependence equally, cylc in effect gets a pool of tasks to
self-organise by negotiating their own dependencies so that optimal
scheduling, as described in the previous section, emerges naturally at
run time.

%\pagebreak
\section{Installation} 
\label{Installation}

\subsection{Requirements} 
\label{Requirements}

\begin{myitemize}
    \item Operating System: Linux or Unix \footnote{The cylc codebase
        assumes Unix-style file paths in places, but it could easily
        made more portable if necessary.} 
    \item Python Version: 2.4 or later, but not Python
        3.x as yet.\footnote{Python 2.4 was released in November 2004. Python 3
        is the future of Python, but it is not backward compatible with
        2.x and consequently still has significantly less library and
        third party support.  As of mid 2011, Python 2.7 is still the
        standard for new Linux distributions.}
    \item PyGTK, a Python wrapper for the GTK+ graphical user interface toolkit.
        PyGTK is included in most Linux Distributions. \newline
        {\em http://www.pygtk.org}
    \item Pyro 3  (latest version tested
        3.14.)\footnote{As of April 2011, Pyro 4, which is compatible
        with Python 3, is in development but it is still not recommended
        for production use.} \newline
        {\em http://irmen.home.xs4all.nl/pyro3}
    \item The graphviz graph layout engine (latest version tested:
        2.28.0). \newline
        {\em http://www.graphviz.org}
    \item Pygraphviz, a python interface to graphviz (latest version
        tested: 1.1).  \newline
        {\em http://networkx.lanl.gov/pygraphviz}
\end{myitemize}

Python and Pyro are essential. PyGTK is required by the gcylc GUI
(but you can control and monitor cylc suites from the command line). 
Graphviz and pygraphviz are required for dependency graphing and the
graph-based suite control GUI (but you can also run cylc without them).

Cylc has absorbed the following software in modified form (no need to
install them): 
\begin{myitemize}
    \item xdot, a graph viewer
        (http://code.google.com/p/jrfonseca/wiki/XDot,
        LGPL license)
    \item ConfigObj and Validate 
        (http://www.voidspace.org.uk/python, %configobj.html,
        BSD license)

\end{myitemize}

\subsection{Unpack The Cylc Tarball}
\label{UnpackingTheCylcTarball}

Cylc installs into a normal user account (although it can be installed
at system-level); just unpack the release tarball in the desired
location, which we shall henceforth call \lstinline=$CYLC_DIR=.
% TO DO: DOCUMENT SYSTEM LEVEL INSTALLATION

\subsection{Configure Your Environment}
\label{ConfiguringYourEnvironmentForCylc}

The file \lstinline=$CYLC_DIR/cylc.profile= documents the minimal user 
environment configuration required for interactive cylc usage:

\lstset{language=bash}
\lstinputlisting{../cylc.profile}
\lstset{language=transcript}

Copy this into your \lstinline=.profile= login script and adapt
appropriately for your environment. After sourcing your modified login
script, or logging in again, you should be able to run cylc:

\begin{lstlisting}
% cylc --version
x.y.z
\end{lstlisting}

\subsection{Create The Central Suite Database}
\label{CreatingTheCentralSuiteDatabase}

Cylc has a central suite database, visible to all users on the 
cylc host, to facilitate sharing of suites. Run the following 
command immediately after installation to create the central database
and export cylc's example suites to it:

\begin{lstlisting}
% cylc admin create-cdb
\end{lstlisting}

To view the content of the resulting central database, run gcylc and
switch to the central database using the Database menu, or 
use \lstinline=cylc db print= on the command line:

\begin{lstlisting}
% cylc db print --central --tree -x 'Auto|Quick'
<admin>
 `-cylc-x:y:z         
   |-AutoCleanup      
   | |-FamilyFailHook  family failure hook script example
   | `-FamilyFailTask  family failure cleanup task example
   |-AutoRecover      
   | |-async           asynchronous automated failure recovery example
   | `-cycling         cycling automated failure recovery example
   `-QuickStart       
     |-a          Quick Start Example A
     |-b          Quick Start Example B
     |-c          Quick Start Example C
     `-z          Quick Start Example Z
\end{lstlisting}
where \lstinline=<admin>= should be replaced with the username of the
cylc installation account on your system. Type
\lstinline=cylc db print --help= to see what the command options
mean.

\subsection{Automated Database Test} 
\label{RTADT}

The command \lstinline=cylc admin test-db= gives suite registration
database functionality a work out - it copies one of the cylc example
suites, registers it under a new name and then manipulates it by
recopying the suite in various ways, exporting it to the central
database, and so on, before finally deleting the test registrations.
This process should complete without error in a few seconds.

\subsection{Automated Scheduler Test} 
\label{RTAST}

The command \lstinline=cylc admin test-suite= tests the cylc scheduler
itself by running a suite temporarily registered under
\lstinline=testsuite=, configuring it to fail out a specific task, and
then doing some advanced failure recovery intervention on it (recursive
purge plus insertion of cold-start tasks). This process should complete
in 2-3 minutes and can be watched in real time by right-clicking on the
temporary test suite when it appears in the gcylc GUI and opening up a
suite control GUI.

\subsection{Complete Non-System-Level Installation}

If you do not have root access to your host machine and cannot easily
get Pyro, Graphviz, and Pygraphviz installed at system level, here's
how to install everything under your home directory. 

Cylc is already designed to be installed into a normal user account -
just unpack the cylc release tarball into \lstinline=$CYLC_DIR=.  Now,
if you try to use cylc commands now you will get a warning that Pyro is
not installed.

Create a new sub-directory in the cylc source tree,
\lstinline=$CYLC_DIR/external=, and download the Pyro,
Graphviz, and Pygraphviz source distributions to it (from the URLs given
at the beginning of Section~\ref{Requirements}).

\subsubsection{Pyro}

Install Pyro under \lstinline=$HOME/external/installed= as follows:
\begin{lstlisting}
% cd $HOME/external
% tar xzf Pyro-3.14.tar.gz
% cd Pyro-3.14
% python setup.py install --prefix=$HOME/external/installed
\end{lstlisting}
Take note of the resulting Python \lstinline=site-packages= directory
under \lstinline=external/installed/=, e.g.:
\begin{lstlisting}
$HOME/external/installed/lib64/python2.6/site-packages/
\end{lstlisting}
The exact path will depend on your local Python environment.
Configure your login script for the cylc environment as described in 
Section~\ref{ConfiguringYourEnvironmentForCylc} above and add in 
this new installation path; for example:
\begin{lstlisting}
# .profile
PYTHONPATH=$HOME/external/installed/lib64/python2.6/site-packages:$PYTHONPATH
PATH=$HOME/external/installed/bin:$PATH
\end{lstlisting}

Now you should be able to get cylc to print its release version: 
\begin{lstlisting}
% . $HOME/.profile   # (or log in again)
% cylc -v
x.y.z
\end{lstlisting}
If this command aborts and says that Pyro is not installed or is not available,
then you have either not installed Pyro (check the output of the installation 
command carefully) {\em or} you have not pointed to the installed Pyro modules
in your PYTHONPATH, {\em or} you have not sourced the cylc environment 
since updating PYTHONPATH.

\subsubsection{Create The Central Suite Database}

Now create the suite database and upload the example suites as described
in Section~\ref{CreatingTheCentralSuiteDatabase}.  At this point you 
should have access to all cylc functionality except for suite graphing
and the graph based suite control GUI. 

\subsubsection{Graphviz}

Install Graphviz under \lstinline=$CYLC_DIR/external/installed= as
follows:
\begin{lstlisting}
% cd $CYLC_DIR/external
% tar xzf graphviz-2.28.0.tar.gz
% cd graphviz-2.28.0
% ./configure --prefix=$CYLC_DIR/external/installed
% make
% make install
\end{lstlisting}
This installs graphviz files into the bin, include, and lib
sub-directories of your local installation directory. The local
installation section of \lstinline=cylc.profile= (above) provides
access to the graphviz executables in this bin directory, although you
will probably not need to use them. The graphviz lib and include
locations are required when installing Pygraphviz (next).

{\em Note that graphviz may fail to build on a system that does not 
have QT installed}. Lack of QT is not important for our purposes and 
you can disable it with \lstinline@./configure --with-qt=no@.

\subsubsection{Pygraphviz}
Install Pygraphviz under \lstinline=$CYLC_DIR/external/installed= as
follows:
\begin{lstlisting}
% cd $CYLC_DIR/external
% tar xzf pygraphviz-1.1.tar.gz
% cd pygraphviz-1.1
\end{lstlisting}
Now edit setup.py lines 31 and 32 to specify the graphviz lib and
include directories:
\begin{lstlisting}
library_path=os.environ['CYLC_DIR'] + '/external/installed/lib'
include_path=os.environ['CYLC_DIR'] + '/external/installed/include/graphviz'
\end{lstlisting}
Or you can just specify the absolute paths if you like, instead of using
the \lstinline=$CYLC_DIR= environment variable. Check that these are the
correct library and include paths by inspecting the contents of the
specified directories, and adjust them if necessary.  Finally, install
pygraphviz:
\begin{lstlisting}
% export CYLC_DIR=/path/to/cylc
% python setup.py install --prefix=$CYLC_DIR/external/installed
\end{lstlisting}
This may or may not, depending on your local Python setup, install the
Pygraphviz modules into the same place as the Pyro modules, e.g.:
\begin{lstlisting}
% ls $CYLC_DIR/external/installed/lib64/python2.6/site-packages/
 pygraphviz  pygraphviz-1.1-py2.6.egg-info  Pyro  Pyro-3.14-py2.6.egg-info
\end{lstlisting}
If not, add the correct Pyraphviz installation path to your PYTHONPATH.

The easiest way to check that pygraphviz has been installed properly is
to start an interactive Python session (type {\em python} after sourcing
the cylc environment to configure your PYTHONPATH) then type {\em import
pygraphviz} at the python interpreter prompt. If this results in the
error message {\em ImportError: No module named pygraphviz} then either
you have not installed pygraphviz properly, {\em or} you have not
configured your PYTHONPATH to point to the installed pygraphviz modules,
{\em or} you have not sourced the cylc environment since updating
PYTHONPATH.  Finally, if you have installed pygraphviz and configured
your PYTHONPATH but graphviz itself has not been installed properly (or
if the graphviz libraries have been deleted since you installed
pygraphviz) then the initial pygraphiz import will be successful but a
lower level import will fail when the pygraphviz modules cannot load the
underlying graphviz libraries - in that case, reinstall graphviz.

\subsubsection{What Next?}

You should now have access to all cylc functionality.  Create 
the central suite database and upload the example suites, if 
you have not done so already
(Section~\ref{CreatingTheCentralSuiteDatabase}), then test 
your cylc installation by running the automated suite database test
(Section~\ref{RTADT}) and the automated scheduler test
(Section~\ref{RTAST}), and go on to the {\em Quick Start Guide}
(Section~\ref{QuickStartGuide}).

\subsection{Upgrading To A New Cylc Version}

Upgrading is just a matter of unpacking the new cylc release,
copying the old central database into the new installation,\footnote{The
central database is currently associated with a particular cylc
installation - it is held in \lstinline=$CYLC_DIR/CDB=.}
and running the new \lstinline=cylc admin create-cdb= to
load the new example suites. Finally, change the newly registered
central suite group name to something sensible with 
\lstinline=cylc db reregister= - it will have an ungainly string of
numbers attached to ensure uniqueness. 

%\pagebreak
\section{Cylc Screenshots}

%The following screenshots provide a glimpse of what the cylc user
%interface looks like.

\begin{myitemize}
    \item Figure~\ref{fig-command-help} - the cylc command line interface.
    \item Figure~\ref{fig-db} - viewing a private suite database in gcylc.
    \item Figure~\ref{fig-cdb} - viewing the central suite database in gcylc.
    \item Figure~\ref{fig-cylc-vim} - a cylc suite definition file (suite.rc) in the vim editor.
    \item Figure~\ref{fig-simple-graph-2} - the suite definition of Figure~\ref{fig-cylc-vim} graphed by cylc.
    \item Figure~\ref{fig-gcylc-treeview} - suite control GUI, treeview interface.
    \item Figure~\ref{fig-gcylc-graph} - suite control GUI, graph-based interface.
    \item Figure~\ref{fig-ecox-1} - a large operational suite graphed by cylc.
    \item Figure~\ref{fig-ecox-2} - another view of the large operational suite of Figure~\ref{fig-ecox-1}.
\end{myitemize}

\begin{figure}
    \begin{center}
        \includegraphics[width=0.8\textwidth]{screenshots/commandhelp.png}
    \end{center}
\caption[The cylc command line interface]{\scriptsize The cylc command line interface}
\label{fig-command-help}
\end{figure} 

\begin{figure}
    \begin{center}
        \includegraphics[width=1.0\textwidth]{screenshots/db.png}
    \end{center}
\caption[gcylc showing a private suite database]{\scriptsize
gcylc showing a private suite database, with one suite running 
on port 7766.}
\label{fig-db}
\end{figure} 

\begin{figure}
    \begin{center}
        \includegraphics[width=1.0\textwidth]{screenshots/cdb.png}
    \end{center}
\caption[gcylc showing the central suite database]{\scriptsize
gcylc showing the central suite database.}
\label{fig-cdb}
\end{figure} 

\begin{figure}
    \begin{center}
        \includegraphics[height=0.35\textheight]{screenshots/simple-suiterc.png}]
    \end{center}
    \caption[A simple cylc suite definition edited in {\em vim}]{\scriptsize
    A simple cylc suite definition edited in {\em vim}}
    \label{fig-cylc-vim} 
\end{figure} 

\begin{figure}
    \begin{center}
        \includegraphics[height=0.35\textheight]{screenshots/simple-18-cold.png}
    \end{center}
    \caption[The suite definition of Figure~\ref{fig-cylc-vim} graphed
    by cylc.]{\scriptsize The suite definition of Figure~\ref{fig-cylc-vim}
    graphed by cylc.}
\label{fig-simple-graph-2}
\end{figure} 

\begin{figure}
    \begin{center}
        \includegraphics[width=0.8\textwidth]{screenshots/control-trad.png}
    \end{center}
\caption[The suite control GUI, treeview interface.]{\scriptsize The suite
control GUI, treeview interface.}
\label{fig-gcylc-treeview}
\end{figure} 

\begin{figure}
    \begin{center}
        \includegraphics[width=0.8\textwidth]{screenshots/control-graph.png}
    \end{center}
\caption[The suite control GUI, graph interface.]{\scriptsize The suite control GUI, graph interface.}
\label{fig-gcylc-graph}
\end{figure} 

\begin{figure}
    \begin{center}
        \includegraphics[width=\textwidth]{screenshots/ecox-1.png}
    \end{center}
\caption[A large operational suite graphed by cylc.]{\scriptsize A large
operational suite graphed by cylc.}
\label{fig-ecox-1}
\end{figure} 

\begin{figure}
    \begin{center}
        \includegraphics[width=0.8\textwidth]{screenshots/ecox-2.png}
    \end{center}
    \caption[The large suite of Figure~\ref{fig-ecox-1} running]
    {\scriptsize The large suite of Figure~\ref{fig-ecox-1} running.}
\label{fig-ecox-2}
\end{figure} 


% dump floats
\clearpage

%\pagebreak

\section{On The Meaning Of {\em Cycle Time} In Cylc}

%You may be used to the idea that a forecasting suite has a ``current
%cycle time'', which is the analysis time or nominal start time
%associated with the main forecast model(s) in the suite, and that
%the whole suite advances to the next forecast cycle when all tasks in
%the current cycle have finished (or even when a particular wall clock
%time is reached, in real time operation). This is not how cylc works.

Cylc suites advance by means of individual tasks independently spawning
successors at the next valid cycle time for the task, not by
incrementing a suite-wide forecast cycle. In fact cylc has no concept of
suite-wide cycle time; instead, each task has its own private cycle time
and can run when its prerequisites are satisfied regardless of other
tasks with different cycle times running at the same time - this
underlies cylc's uniquely efficient and flexible scheduling abilities.
However, it may sometimes still be convenient to refer to the ``current
cycle'', the ``previous cycle'', or the ``next cycle'' and so forth,
with reference to a particular task, or in the sense of all tasks that
``belong to'' a particular forecast cycle.  Just keep in mind that the
members of such groups may not all exist simultaneously in the running
suite - tasks may pass through the ``current cycle'' (etc.) at different
times as the suite evolves, particularly in delayed (catch up)
operation.


\section{Cylc's Example Suites}

Cylc has been designed from the ground up to make prototyping and testing
new suites very easy. Simply drawing (in text) a dependency graph in a
new suite definition creates a valid suite that you can run (the tasks
will be {\em dummy tasks} that default to emitting an identifying
message, sleeping for a few seconds, and then exiting; but you can then
arrange for particular tasks to do more complex things by configuring
task runtime properties appropriately).

Cylc has quite a number of example suites intended to illustrate most
facets of suite construction. These are held centrally under
\lstinline=$CYLC_DIR/examples= and are exported to the central suite
database, accessible to all users, when cylc is installed. They all run
``out the box" and can be copied and modified by users to test almost
anything.  Some of them just configure a suite dependency graph, in
which case cylc will run dummy tasks according to the graph; some also
configure task runtime properties (e.g.\ command scripting and
environment variables) within the suite definition; and some have real
task implementations that generate and consume real files and which will
fail if they are not executed in the right order.  All of the example
suites use their registration names, portably via suite and task
identity variable supplied by cylc (this is in fact the default
situation) in all suite and task I/O paths, so you can run multiple
copies of the same example suite at once, under different names, without
changing anything in the suite definition.

\subsection{Using The Example Suites}

Use \lstinline=gcylc= or \lstinline=cylc db print --central <admin>= to
see the example suites that are currently available. Note that here, 
and throughout the User Guide, any reference to an example suite in
the central database will use \lstinline=<admin>= as the username of the
account under which cylc is installed (the suite owner's username is
always the first component of a suite name in the central database);
replace this as appropriate for your local cylc installation. Now 
use \lstinline=cylc import= to copy an example suite (or some subset of 
them, or all of them at once) to your private database so that you can
modify it and run it:
\begin{lstlisting}
% cylc db print --central --tree <admin>.*\.QuickStart
# or just this:
% cylc db print --central --tree QuickStart
<admin>         
 `-cylc-x:y:z   
   `-QuickStart 
     |-a   Quick Start Example A | ~/cylc/CDB/<admin>/cylc-x:y:z/QuickStart/a
     |-b   Quick Start Example B | ~/cylc/CDB/<admin>/cylc-x:y:z/QuickStart/b
     |-c   Quick Start Example C | ~/cylc/CDB/<admin>/cylc-x:y:z/QuickStart/c
     `-z   Quick Start Example Z | ~/cylc/CDB/<admin>/cylc-x:y:z/QuickStart/z
% cylc db import <admin>.cylc-x:y:z.QuickStart.z eg.z $TMPDIR
COPY /home/<admin>/cylc/CDB/<admin>/cylc-x:y:z/QuickStart/z 
  TO /tmp/oliver/eg/z
REGISTER eg.z: /tmp/oliver/eg/z

% cylc db print eg
eg.z | Quick Start Example Z | /tmp/oliver/eg/z

% cylc prep edit eg.z
\end{lstlisting}

\lstset{language=suiterc}
\lstinputlisting{../examples/QuickStart/z/suite.rc}
\lstset{language=transcript}
(This suite is explained in the {\em Quick Start Guide},
Section~\ref{QuickStartGuide}).


\subsection{Choosing The Initial Cycle Time}

When running any suite in live mode (or at least suites that depend on
clock-triggered tasks) do not give an initial cycle time in the future
unless you want nothing to happen until that time. However, you can also
run any suite in {\em simulation mode} (Section~\ref{SimulationMode}) in
which case a future start time is fine. In simulation mode each task is
replaced by the default dummy task (above) and the suite runs quickly on
an accelerated clock. As far as cylc is concerned this is almost
identical to real operation, so simulation mode can be used to test
recovery strategies for certain kinds of failure, for instance. In
simulation mode, by configuring how the accelerated clock is
initialized, you can watch how any suite catches up and transitions from
delayed to real time operation.

%\pagebreak
\section{Suite Registration}
\label{SuiteRegistration}

Before you can do anything to a suite with cylc commands you must
register it in a {\em suite database}. Cylc commands target 
particular suites via their registered names so that you don't
need to remember and continually re-type the actual location of the
suite definition directory on disk. Suite names are hierarchical like
directory paths but delimited by `.' (foo.bar.baz), allowing suites to
be grouped and organised into trees (c.f.\ directory trees). 
Task groups are virtual and do not need to be explicitly created before
use or removed if they contain no tasks. 

\subsection{Private Suite Databases}

Each cylc user has a private suite database for working with his own
suites. By right-clicking on a suite in your private database 
(or using cylc commands) you can:
\begin{myenumerate}
    \item start a suite control GUI to run the suite (or connect to a running suite),
    \item submit a single task to run, just as it would be submitted by its suite
    \item view the suite stdout and stderr streams,
    \item view the suite log (which records all events and messages from tasks),
    \item retrieve the suite description,
    \item list tasks in the suite,
    \item view the suite namespace hierarchy,
    \item edit the suite definition in your editor,
    \item plot the suite dependency graph,
    \item search the suite definition and bin scripts,
    \item validate the suite definition,
    \item copy the suite or group,
    \item alias the suite name to another name,
    \item compare (difference) the suite with another suite,
    \item export the suite or group to the central database,
    \item unreregister the suite or group,
    \item reregister the suite or group.
\end{myenumerate}

Note that the suite title shown in gcylc is parsed from the suite.rc
file at the time of initial registration; if you change the title (by
editing the suite.rc file) use \lstinline=cylc db refresh= or gcylc 
View $\rightarrow$ Refresh to update the database.

\subsection{The Central Suite Database}

The central suite database facilitates sharing of suites among cylc
users. It works similarly to a private suite database except that
central suite registration names always begin with the suite owner's
username.  Users can export suites to the central database to make them
available to others.

Suites in the central database can be inspected by various means, but
you can't run them without {\em importing} them to your private database.

By right-clicking on a suite in the central database, or using the cylc
command line, you can:
\begin{myenumerate}
    \item retrieve the suite description,
    \item list tasks in the suite,
    \item view the suite namespace hierarchy,
    \item view the suite definition in your editor,
    \item plot the suite dependency graph,
    \item search the suite definition and bin scripts,
    \item validate the suite definition,
    \item compare (difference) the suite with another suite,
    \item import the suite or group to your private database,
    \item unreregister or delete the suite or group (if you own it),
    \item reregister the suite or group (if you own it).
\end{myenumerate}

\subsubsection{The Central Database Is Local To The Cylc Host}

The central suite database is not currently accessible on the
network, it is local to the cylc host and accessed through the 
filesystem. Consequently it has to be world, or at least group,
writeable, which is not very secure.

%Also it briefly locks users out during operations that change the
%        minimal
%registration data (i.e.\ the mappings between suite names and locations,
%not when copying suite definitions to the central location, which is
%potentially time consuming). 

{\em We intend to develop a client/server interface to the central suite
database so that users can easily share suites across the network}. The 
required server functionality is similar to that of the cylc lockserver
(and to cylc itself) so this will not be difficult. In the meantime,
``importing'' a suite manually from another user on another host is
simply a matter of copying the suite definition directory over and
registering the new copy in your private database.

\subsection{Database Operations}

On the command line, the  `database' (or `db') command category contains
commands to implement the aforementioned operations.

\lstset{language=usage}
\lstinputlisting{command-usage/database.txt}
\lstset{language=transcript}
Groups of suites (at any level in the registration hierarchy) can be deleted, 
copied, imported, and exported; as well as individual suites.  To do this, 
just use suite group names as source and/or target for operations, as
appropriate.  For instance, if a group \lstinline=foo.bar= contains the 
suites \lstinline=foo.bar.baz= and \lstinline=foo.bar.waz=, you can copy
a single suite like this:
\begin{lstlisting}
% cylc copy foo.bar.baz boo $TMPDIR
\end{lstlisting}
(resulting in a new suite \lstinline=boo=); or the group like this:
\begin{lstlisting}
% cylc copy foo.bar boo $TMPDIR
\end{lstlisting}
(resulting in new suites \lstinline=boo.baz= and \lstinline=boo.waz=); or the group like this:
\begin{lstlisting}
% cylc copy foo boo $TMPDIR
\end{lstlisting}
(resulting in new suites \lstinline=boo.bar.baz= and \lstinline=boo.bar.waz=).
When suites are copied, the suite definition directories are copied into
a directory tree, under the target directory, that reflects the
registration name hierarchy. \lstinline=cylc copy --help= has some explicit examples.



The same functionality is also available by right-clicking on suites
or groups in the gcylc GUI, as shown in Figure~\ref{fig-db}.

%\pagebreak
\section{Quick Start Guide} 
\label{QuickStartGuide}

This section works through some basic cylc functionality using the
``QuickStart'' example suites, which should have been registered in the
central suite database, under the cylc admin username (replace 
\lstinline=<admin>= below with the correct username for your system)
during cylc installation, 

\begin{lstlisting}
% cylc db print --central --tree QuickStart
<admin>         
 `-cylc-x:y:z   
   `-QuickStart 
     |-a        Quick Start Example A | ~/cylc/CDB/<admin>/cylc-x:y:z/QuickStart/a
     |-b        Quick Start Example B | ~/cylc/CDB/<admin>/cylc-x:y:z/QuickStart/b
     |-c        Quick Start Example C | ~/cylc/CDB/<admin>/cylc-x:y:z/QuickStart/c
     `-z        Quick Start Example Z | ~/cylc/CDB/<admin>/cylc-x:y:z/QuickStart/z
\end{lstlisting}

\subsection{Configure Your Environment}

To get access to cylc you just need to set a few environment variables
in your login script, as described in
Section~\ref{ConfiguringYourEnvironmentForCylc}.

\subsection{Starting The gcylc GUI}

At the command prompt:
\begin{lstlisting}
% gcylc &
\end{lstlisting}
and use the Database menu to switch to the central database, which
displays suites accessible to all users (see Figure~\ref{fig-cdb}).

\subsection{Import The QuickStart Suites} 

You can register new suites directly in your private database, via
gcylc or \lstinline=cylc db register=, or you can import suites
from the central database that is visible to all users. Suites in 
the central database can be view, validated, and graphed, etc., 
but you have to import them in order to run them. 
Section~\ref{SuiteRegistration} explains exactly what actions can be 
performed on suites in central and private databases.

In gcylc, find the Quick Start suites (use View $\rightarrow$ Filter to
filter for ``QuickStart'' if you like), right click on the {\em
QuickStart} group and choose `Import' to copy the whole group to your
private database. In the dialog box that pops up, enter `QuickStart' 
as the TARGET suite name, and \lstinline=$TMPDIR= (or some other
directory of your choice) as a destination for the suite definition
directories in the group.  Equivalently, on the command line:
\begin{lstlisting}
% cylc db import <admin>.cylc-x:y:z.QuickStart Quickstart $TMPDIR
COPY /home/<admin>/cylc/CDB/<admin>/cylc-x:y:z/QuickStart/a 
  TO /tmp/oliver/QuickStart/a
REGISTER QuickStart.a: /tmp/oliver/QuickStart/a
COPY /home/<admin>/cylc/CDB/<admin>/cylc-x:y:z/QuickStart/b 
  TO /tmp/oliver/QuickStart/b
REGISTER QuickStart.b: /tmp/oliver/QuickStart/b
COPY /home/<admin>/cylc/CDB/<admin>/cylc-x:y:z/QuickStart/c 
  TO /tmp/oliver/QuickStart/c
REGISTER QuickStart.c: /tmp/oliver/QuickStart/c
\end{lstlisting}

Note that you can also import individual suites, and that
registration groups do not need to be created explicitly - they
are automatically created and deleted as required. 

Now switch gcylc back to your private database and confirm that you have a
copy of the example suites; on the command line:
\begin{lstlisting}
% cylc db pr -t Quick
QuickStart    
 |-a       Quick Start Example A | /tmp/oliver/QuickStart/a
 |-b       Quick Start Example B | /tmp/oliver/QuickStart/b
 |-c       Quick Start Example C | /tmp/oliver/QuickStart/c
 `-z       Quick Start Example Z | /tmp/oliver/QuickStart/z
\end{lstlisting}

\subsection{View The QuickStart.a Suite Definition}

Cylc suites are defined by {\em suite.rc files}, discussed at length in
{\em Suite Definition} (Section~\ref{SuiteDefinition}) and the {\em
Suite.rc Reference} (Appendix~\ref{SuiteRCReference}).  To view the
Quickstart.a suite definition right-click on the suite
name and choose `Edit'; or use the edit command:
\begin{lstlisting}
% cylc edit QuickStart.a
\end{lstlisting}

This opens the suite definition in your editor (\lstinline=$EDITOR=, or
\lstinline=$GEDITOR= from gcylc) {\em from the suite definition
directory so that you can easily open other suite files in the
editor}. You can of course do this manually, but by using the cylc
interface you don't have to remember suite definition directory
locations. For the rare occasions that you do need to move to a 
suite definition directory, you can do this:
\begin{lstlisting}
% cd $( cylc db get-dir QuickStart.a )
\end{lstlisting}
Suites that use include-files can optionally be edited in a temporarily
inlined state - the inline file will be split back into its constituent
include-files when you save it and exit the editor. 

Anyhow, you should now see the following suite.rc file in your editor: 
\lstset{language=suiterc}
\lstinputlisting{../examples/QuickStart/a/suite.rc}
\lstset{language=transcript}
(Cylc comes with syntax highlighting and section folding for the {\em vim}
editor - see Section~\ref{SyntaxHighlighting}).

This defines a complete, valid, runnable suite.  Here's how to interpret
it: At 0, 6, 12, and 18 hours each day a clock-triggered task called GetData
triggers 1 hour after the wall clock reaches its (GetData's) nominal cycle
time; then a task called Model triggers when GetData finishes;
and a task called PostA triggers when Model is finished. Additionally,
Model depends on its own previous instance from 6 hours earlier; and twice
per day at 6 and 18 hours another task called PostB also triggers off Model.

All the tasks in this suite can run in parallel with their own previous
instances if the opportunity arises (i.e.\ if their prerequisites
are satisfied before the previous instance is finished). Most tasks
should be capable of this (see Section~\ref{LimitPID}) but if
necessary you can force particular tasks to run sequentially like 
this:
\lstset{language=suiterc}
\begin{lstlisting}
# SUITE.RC
[scheduling]
    [[special tasks]]
        sequential = GetData, PostB
\end{lstlisting}
\lstset{language=transcript}

Finally, when the suite is {\em cold-started} (started from scratch) it
is made to wait on a special {\em synchronous start-up task} called
Prep. Start-up tasks are one-off (non-spawning) tasks that are only used
at suite start-up, and any dependence on them only applies at suite
start-up. Start-up tasks are {\em synchronous} because they have a 
defined cycle time even though they are not cycling tasks. Cylc also has
{\em asynchronous one-off tasks}, which have no cycle time:
\lstset{language=suiterc}
\begin{lstlisting}
# SUITE.RC
[scheduling]
    [[dependencies]]
        graph = "prep"     # an asynchronous one-off task (no cycle time)
        [[[ 0,6,12,18 ]]]
            graph = "prep => foo => bar"   # followed by cycling tasks
\end{lstlisting}
\lstset{language=transcript}

The optional visualization section configures graph plotting.

\subsection{Plotting The QuickStart.a Dependency Graph}

Right-click on the {\em QuickStart.a} suite in gcylc and choose Graph;
or by command line,
\begin{lstlisting}
% cylc graph QuickStart.a 2011052300 2011052318 &
\end{lstlisting}
This will pop up a zoomable, pannable, graph viewer showing
the graph of Figure~\ref{fig-QuickStartA-graph18}. 
If you edit the suite.rc file the viewer will update in real time
whenever you save the file.

\begin{figure}
    \begin{center}
        \includegraphics[width=0.7\textwidth]{screenshots/QuickStartA-graph18.png}
    \end{center}
    \caption[The {\em QuickStart.a} dependency graph]{\scriptsize The
    {\em QuickStart.a} dependency graph, plotted by cylc.}
\label{fig-QuickStartA-graph18}
\end{figure}

\subsection{Run The QuickStart.a Suite}

Each cylc task defines command scripting to invoke the right external
processing when the task is ready to run. This has not been explicilty
configured in the example suite, so it defaults, for all tasks, to the
{\em dummy task} scripting inherited from the {\em root namespace} (see
Section~\ref{SuiteDefinition}):
\begin{lstlisting}
% cylc get-config QuickStart.a runtime GetData 'command scripting'
['echo DUMMY $CYLC_TASK_ID; sleep 10; echo BYE']
\end{lstlisting}
where \lstinline=$CYLC_TASK_ID= is the unique identifier by which 
cylc knows the task (\lstinline=NAME%CYCLE=) exported to each task's
execution environment by cylc. The command arguments reflect the suite
definition section nesting. If it were explicitly configured in the 
example suite it would look like this:
\lstset{language=suiterc}
\begin{lstlisting}
[runtime]
    [[GetData]]
        command scripting = "echo DUMMY $CYLC_TASK_ID; sleep 10; echo BYE"
\end{lstlisting}
\lstset{language=transcript}

Now start a suite control GUI by right-clicking on the suite in gcylc
and choosing `Control (graph)'.  You can also open a text treeview
control GUI for the same suite, if you like. Multiple GUIs running at
the same time will automatically connect to the same running suite (they
won't try to run separate instances). Note also that if you shut down a
suite control GUI, the suite will keep running. You can reconnect to it
later by opening another control GUI.
%See {\em The Graph-Based Suite Control GUI},
%Section~\ref{TheGraphBasedcontrolGUI}.

In the control GUI click on Control $\rightarrow$ Run, enter an initial
cold-start cycle time (e.g.\ 2011052306), and select ``Hold (pause) on
start-up'' so that the suite will start in the held state (tasks
will not be submitted even if they are ready to run). 

{\em Do not choose an initial cycle time in the future unless you're
running in simulation mode, or nothing much will happen until that
time.}

If the initial cycle time ends in 06 or 18 
the suite controller should look like
Figure~\ref{fig-QuickStartA-ControlStart06},
or otherwise (00 or 12) like
Figure~\ref{fig-QuickStartA-ControlStart00}.

\begin{figure}
    \begin{center}
        \includegraphics[width=0.8\textwidth]{screenshots/QuickStartA-ControlStart06.png}
    \end{center}
    \caption[Suite {\em QuickStart.a} at start-up, 06 or 18 hours.]{\scriptsize
    Suite {\em QuickStart:one} at start-up with an initial
    cycle time ending in 06 or 18 hours. Yellow nodes represent 
    waiting tasks in the held state.}
    \label{fig-QuickStartA-ControlStart06}
\end{figure} 

\begin{figure}
    \begin{center}
        \includegraphics[width=0.8\textwidth]{screenshots/QuickStartA-ControlStart00.png}
    \end{center}
    \caption[Suite {\em QuickStart.a} at start-up, 00 or 12
    hours]{\scriptsize Suite {\em QuickStart.a} at start-up with an
    initial cycle time ending in 00 or 12 hours. Yellow nodes represent 
    waiting tasks in the held state and off-white nodes are tasks from
    the base graph, defined in the suite.rc file, that aren't currently
    live in the suite.}
    \label{fig-QuickStartA-ControlStart00}
\end{figure} 

The reason for the difference in graph structure between the two figures
is this: cylc starts up with every task present in the waiting
state (blue) at the initial cycle time {\em or} at the first
subsequent valid cycle time for the task - and PostB does not run at 00
or 12. The off-white tasks are from the base graph, defined in the 
suite.rc file, and aren't actually present in the suite as yet (they
are shown in the graph in order to put the live tasks in context).

Now, click on Control $\rightarrow$ Release in the suite control GUI
to {\em release the hold on the suite}, 
and observe what happens: the GetData tasks will rapidly go off 
in parallel out to a few cycles ahead (how far ahead depends on 
the suite runahead limit, explained below) and then the suite will
stall, as shown in Figures~\ref{fig-QuickStartA-ControlRunning}
and~\ref{fig-QuickStartA-ControlStalled}. 

\begin{figure}
    \begin{center}
        \includegraphics[width=0.8\textwidth]{screenshots/QuickStartA-ControlRunning.png}
    \end{center}
    \caption[Suite {\em QuickStart.a} running.]
    {\scriptsize Suite {\em QuickStart.a} running, showing several
    consecutive instances of the clock-triggered GetData task running at
    once, out to the suite runahead limit of 12 hours.}
    \label{fig-QuickStartA-ControlRunning}
\end{figure} 
\begin{figure}
    \begin{center}
        \includegraphics[width=0.8\textwidth]{screenshots/QuickStartA-ControlStalled.png}
    \end{center}
    \caption[ Suite {\em QuickStart.a} stalled.]
    {\scriptsize Suite {\em QuickStart.a} stalled after the
    clock-triggered GetData tasks have finished, because of Model's
    previous-cycle dependence and the suite runahead limit (see main
    text).}
    \label{fig-QuickStartA-ControlStalled}
\end{figure} 

The Prep task runs immediately because it has no prerequisites and is
not clock-triggered. The clock-triggered GetData tasks then all go off
at once because they have no prerequisites (i.e.\ they do not have to
wait on any upstream tasks), their trigger time has long passed (the
initial cycle time was in the past), and they are not sequential tasks
(so they are able to run in parallel - try declaring GetData
sequential to see the difference).
Beyond the suite {\em runahead limit} of 12 hours (set in the suite.rc
file), however, GetData is put into a special `runahead' held state 
indicated by the darker blue graph node. The task will be released 
from this state once the slower tasks in the suite have caught up 
sufficiently.  The runahead limit is designed to stop free tasks like
this from running off too far into the future in delayed operation. It
is of little conseqence in real time operation\footnote{So long as the
runahead limit is sufficient to span the normal range of cycle times
present in the suite - task that only run once per day, for example,
have to spawn a successor that is 24 hours ahead.} because clock
triggered tasks are then constrained by the wall clock, and other tasks
have to wait on them, generally speaking.

\subsubsection{Viewing The State Of Tasks}

If you're wondering why a particular task has not triggered yet in a
running suite you can view the current state of its prerequisites 
by right-clicking on the task and choosing `View State', or using
\lstinline=cylc show=. Do this for the first Model task, which appears 
to be stuck in the waiting state; it will pop up a small window as in   
Figure~\ref{fig-QuickStartA-ModelState}.

\begin{figure}
    \begin{center}
        \includegraphics[width=0.4\textwidth]{screenshots/QuickStartA-ModelState.png}
    \end{center}
    \caption[Viewing current task state in gcylc]
{\scriptsize Viewing current task state after right-clicking on a task in gcylc. The
    same information is available from the \lstinline=cylc show= command.}
    \label{fig-QuickStartA-ModelState}
\end{figure} 

It is clear that the reason the task is not running, and consequently,
by virtue of the runahead limit, why the suite has stalled, is
that Model[T] is waiting on Model[T-6] which does not exist at suite
start-up. Model represents a warm-cycled forecast model that depends on a
model background state or restart file(s) generated by its own
previous run.

\subsubsection{Triggering Tasks Manually}

Right-click on the waiting Model task and choose Trigger, or use
\lstinline=cylc trigger=, to force the task to trigger, and thereby get
the suite up and running. In a real suite this would not be sufficient:
the real forecast model that Model represents would fail for lack of the
real restart files that it requires as input.  Well see how to handle
this properly shortly.

\subsubsection{Suite Shut-Down And Restart}

Having watched the {\em QuickStart.a} suite run for a while, choose
Stop from the Control menu, or \lstinline=cylc stop=, to shut it down.
The default stop method waits for any tasks that are currently running
to finish before shutting the suite down, so that the final recorded
suite state is perfectly consistent with what actually happened.  

You can restart the suite from where it left off by choosing 
Control $\rightarrow$ Run and selecting the `restart' option, or using
\lstinline=cylc restart=. Note that cylc always writes a special
state dump, and logs its name, prior to actioning any intervention, and
you can also restart a suite from one of these states, rather than the 
default most recent state.

\subsection{QuickStart.b - Handling Cold-Starts Properly}

Now take a look at {\em QuickStart.b}, which is a minor
modification of {\em QuickStart.a}. Its suite.rc file has a new
{\em cold-start} task called ColdModel,
\lstset{language=suiterc}
\begin{lstlisting}
# SUITE.RC
[scheduling]
    [[special tasks]]
        cold-start = ColdModel
\end{lstlisting}
\lstset{language=transcript}
and the dependency graph (see also Figure~\ref{fig-QuickStartB-graph18})
looks like this:
\lstset{language=suiterc}
\begin{lstlisting}
# SUITE.RC
[scheduling]
    [[dependencies]]
        [[[ 0,6,12,18 ]]]
            graph  = """Prep => GetData & ColdModel
                        GetData => Model => PostA
                        ColdModel | Model[T-6] => Model"""
        [[[ 6,18 ]]]
            graph = "Model => PostB"
\end{lstlisting}
\lstset{language=transcript}
In other words, Model[T] can trigger off {\em either} Model[T-6] {\em or} 
ColdModel[T]. 

\begin{figure}
    \begin{center}
        \includegraphics[width=0.7\textwidth]{screenshots/QuickStartB-graph18.png}
    \end{center}
    \caption[The {\em QuickStart.b} graph with model cold-start task.]{\scriptsize
    The {\em QuickStart.b} dependency graph showing a model cold
    start task.}
\label{fig-QuickStartB-graph18}
\end{figure}

A cold-start task is a one-off task used to satisfy the previous-cycle
dependence of a cotemporal task whose previous-cycle trigger is not
available. The obvious use for this is to cold-start warm-cycled
forecast models at suite start-up, when there is no previous cycle.
Unlike start-up tasks though, cold-start dependencies are not restricted
to suite start-up because it is sometimes useful to be able to 
insert a cold-start task into a running suite, to get a model restarted
after it had to skip one or more cycles due to problems, without having
to restart the whole suite.

A model cold-start task in a real suite may submit a real ``cold
start forecast'' to generate the previous-cycle input files required by
its associated model, or it may just stand in for some external
spinup process, or similar, that has to be completed before the suite
is started (in the latter case the cold-start task would be a dummy task 
that just reports successful completion in order to satisfy the initial
previous-cycle dependence of the model).

Run {\em QuickStart.b} to confirm that that no manual triggering is
required to get the suite started now.

\subsection{QuickStart.c - Real Task Implementations}

The suite {\em QuickStart.c} is the same as {\em QuickStart.b}
except that it has real task implementations (scripts located
in the suite bin directory) that generate and consume files in such a
way that they have to run according to the graph of
Figure~\ref{fig-QuickStartB-graph18}. The suite gets them to run
together out of a common I/O workspace, configured via the suite.rc
file. 

By studying this suite and its tasks, and by making quick copies of 
it to modify and run, you should be able to learn a lot about how 
to build real cylc suites. Here's the complete suite definition 
\lstset{language=suiterc}
\lstinputlisting{../examples/QuickStart/c/suite.rc}
\lstset{language=transcript}
Here's the namespace hierarchy defined by this suite:
\begin{lstlisting}
% cylc list --tree QuickStart.c
root           
 |-GetData     retrieve data for the current cycle time
 |-Models      
 | |-ColdModel cold start the forecast model
 | `-Model     the forecast model
 |-Post        
 | |-PostA     post processing for model
 | `-PostB     post processing for model
 `-Prep        prepare the suite workspace for a new run
\end{lstlisting}
And here, for example, is the complete implementation for the PostA task
(located with the other task scripts in the suite bin directory):
\lstset{language=bash}
\lstinputlisting{../examples/QuickStart/c/bin/PostA.sh}
\lstset{language=transcript}

\subsection{Monitoring Running Suites}

\subsubsection{Suite stdout And stderr}

When cylc runs a suite it writes warnings and other informative messages,
such as when and how each task is submitted, to the stdout stream. If 
you start a suite at the command line you can direct this output wherever
you like. If you start a suite via gcylc, however, the output is 
directed to a special file,
\lstinline=$HOME/.cylc/[SUITE].out= that can be accessed again later if
you reconnect to the suite with a new control GUI.

\begin{figure}
    \begin{center}
        \includegraphics[width=0.7\textwidth]{screenshots/suite-output.png}
    \end{center}
\caption[Cylc suite stdout/stderr example.]{\scriptsize Cylc suite stdout/stderr example.}
\label{fig-suite-output}
\end{figure}

\subsubsection{Suite Logs}

The cylc suite log records every event that occurs (incoming messages from tasks
and so on) along with the time of the event. The top level logging directory,
under which a suite-specific log is written, is configurable in the suite.rc file.
Suite logs are (optionally) rolled over at start-up and the five most
recent back ups are kept.

\begin{figure}
    \begin{center}
        \includegraphics[width=0.7\textwidth]{screenshots/suite-log.png}
    \end{center}
\caption[ A cylc suite log viewed via gcylc.]{\scriptsize A cylc suite log viewed via gcylc.}
\label{fig-suite-log}
\end{figure}

Figure~\ref{fig-suite-log} shows a suite log viewed from within gcylc. The 
\lstinline=cylc log= command also enables viewing and filtering of suite logs
without having to remember the actual log file location. But if you want 
to know the location:
\begin{lstlisting}
% cylc get-config --directories QuickStart.a
SUITE LOG DIRECTORY:
  /home/oliverh/cylc-run/QuickStart.a/log/suite
SUITE STATE DUMP DIRECTORY:
  /home/oliverh/cylc-run/QuickStart.a/state
JOB SUBMISSION LOG DIRECTORIES:
  + root: /home/oliverh/cylc-run/QuickStart.a/log/job
\end{lstlisting}
(There would be other job submission log directories listed here if 
any of the tasks in the suite had overridden the default location 
set by the root namespace).

\subsubsection{Task stdout And stderr Logs}

\lstset{language=transcript}
The stdout and stderr logs generated when a task is submitted end up in
the suite.rc {\em job submission log directory}, default location
\lstinline=$HOME/cylc-run/$CYLC_SUITE_REG_NAME/log/job/= (where the
variable \lstinline=$CYLC_SUITE_REG_NAME= evalutes to the registered
suite name hierarchy, e.g.\ foo.bar.baz): 
\begin{lstlisting}
% get-config QuickStart.a runtime GetData 'job submission' 'log directory'
/home/oliverh/cylc-run/QuickStart.a/log/job
\end{lstlisting}
(or use the \lstinline=--directories= argument as shown just above).

These files will contain the complete stdout and stderr record for tasks
whose initiating processes do not detach and exit before all task
processing is finished. The location of output generated by secondary
processes, if the original process detaches and exits early, is up to
the task implementation (of said secondary processes, specifically).

\subsection{Searching A Suite}

The cylc suite search tool reports matches in the suite.rc file
by line number, suite section, and file, even if include-files
are used (and even if they are nested), and by file and line number for
matches in the suite bin directory. The following output listing is from
a search of the {\em QuickStart.c} suite. 
\begin{lstlisting}
% cylc grep OUTPUT_DIR QuickStart.c
\end{lstlisting}
\lstset{language=suiterc}
\begin{lstlisting}
SUITE: QuickStart.c /tmp/oliverh/QuickStart/c/suite.rc
PATTERN: OUTPUT_DIR

FILE: /tmp/oliverh/QuickStart/c/suite.rc
   SECTION: [runtime]->[[GetData]]->[[[environment]]]
      (40):             GETDATA_OUTPUT_DIR = $WORKSPACE
   SECTION: [runtime]->[[Models]]->[[[environment]]]
      (45):             MODEL_OUTPUT_DIR = $WORKSPACE
   SECTION: [runtime]->[[Post]]->[[[environment]]]
      (60):             OUTPUT_DIR = $WORKSPACE

FILE: /tmp/oliverh/QuickStart/c/bin/PostB.sh
   (7): cylc checkvars -c OUTPUT_DIR
   (21): touch $OUTPUT_DIR/precip.products

FILE: /tmp/oliverh/QuickStart/c/bin/Model.sh
   (11): cylc checkvars -c MODEL_OUTPUT_DIR MODEL_RUNNING_DIR
   (54): touch $MODEL_OUTPUT_DIR/surface-winds-${CYLC_TASK_CYCLE_TIME}.nc
   (55): touch $MODEL_OUTPUT_DIR/precipitation-${CYLC_TASK_CYCLE_TIME}.nc

FILE: /tmp/oliverh/QuickStart/c/bin/PostA.sh
   (7): cylc checkvars -c OUTPUT_DIR
   (21): touch $OUTPUT_DIR/surface-wind.products

FILE: /tmp/oliverh/QuickStart/c/bin/GetData.sh
   (6): cylc checkvars -c GETDATA_OUTPUT_DIR
   (11): touch $GETDATA_OUTPUT_DIR/obs-${CYLC_TASK_CYCLE_TIME}.nc
\end{lstlisting}
(Suite search is also available from the gcylc right-click menu).
\lstset{language=transcript}

\subsection{Comparing Suites}

Cylc can also compare suites and report differences by suite.rc 
section and item. For instance, comparing the example suites 
\lstinline=QuickStart.a= and \lstinline=QuickStart.b= by GUI or
command line results in:
\lstset{language=transcript}
\begin{lstlisting}
% cylc diff QuickStart.a QuickStart.b
\end{lstlisting}
\lstset{language=suiterc}
\begin{lstlisting}
Parsing QuickStart.a
Parsing QuickStart.b
Suite definitions QuickStart.a and QuickStart.b differ.
!SNIP!
13 common items differ QuickStart.a(<) QuickStart.b(>)
   (top)
 <   description = (see the Cylc User Guide)
 >   description = (Quick Start a plus a cold-start task)
 <   title = Quick Start Example A
 >   title = Quick Start Example B

   [cylc] [[logging]]
 <   directory = /home/oliverh/cylc-run/QuickStart.a/log/suite
 >   directory = /home/oliverh/cylc-run/QuickStart.b/log/suite

   [scheduling] [[dependencies]] [[[0,6,12,18]]]
 <   graph = Prep => GetData => Model => PostA
                        Model(T-6) => Model
 >   graph = Prep => GetData & ColdModel
                        GetData => Model => PostA
                        ColdModel | Model(T-6) => Model
!SNIP!
\end{lstlisting}
\lstset{language=transcript}
(much of the diff output has been omitted here for the sake of brevity).
Note that suite log directories and the like may differ even though they
are not explicitly configured in either suite, because their default
values (see the {\em Suite.rc Reference},
Appendix~\ref{SuiteRCReference}) are suite-registration-specific.

\subsection{Validating A Suite}

Suite validation checks for errors by parsing the suite definition,
comparing all items against the suite.rc specification file, and then
parsing the suite graph and attempting to instantiate all task proxy
objects. This can be done using gcylc or \lstinline=cylc validate=:
\lstset{language=transcript}
\begin{lstlisting}
% cylc validate -v foo.bar
Parsing Suite Definition
LOADING suite.rc
VALIDATING against the suite.rc specification.
PARSING clock-triggered tasks
PARSING runtime generator expressions
PARSING runtime hierarchies
PARSING SUITE GRAPH
Instantiating Task Proxies:
root              
 |-GEN            
 | |-OPS          
 | | |-aircraft    ... OK
 | | |-atovs       ... OK
 | | `-atovs_post  ... OK
 | `-VAR          
 |   |-AnPF        ... OK
 |   `-ConLS       ... OK
 |-baz            
 | |-bar1          ... OK
 | `-bar2          ... OK
 |-foo             ... OK
 `-prepobs         ... OK
Suite foo.bar validates OK.
\end{lstlisting}

For more information on suite validation see
Section~\ref{Validation}.

\section{Suite Definition} 
\label{SuiteDefinition}

A cylc suite is defined entirely by a single structured, validated,
configuration file called {\em suite.rc} that concisely specifies the
properties of, and the relationships between, the various tasks managed
by the suite. This section of the User Guide deals with the format and
content of the suite.rc file, including task definition.  Task
implementation - what's required of the real commands, scripts, or
programs that do the processing that the tasks represent - is covered in
Section~\ref{TaskImplementation}; and task job submission - how tasks 
are submitted to run - is in Section~\ref{TaskJobSubmission}.

\subsection{Suite Definition Directories}

A cylc {\em suite definition directory} contains:
\begin{myitemize}
    \item {\bf A suite.rc file}: this is {\em the} suite definition.
        \begin{myitemize}
            \item And any include-files used in it (see below; may be
                kept in sub-directories).
        \end{myitemize}
    \item {\bf A suite \lstinline=bin/= directory}.
        \begin{myitemize}
            \item For scripts and executables that implement, or are
                used by, suite tasks.
            \item Automatically added to \lstinline=$PATH= in task
                execution environments.
            \item Technically optional as tasks can call external
                commands, scripts, or programs; or they can be scripted 
                entirely within the suite.rc file.
        \end{myitemize}
    \item {\bf Any other sub-directories and files} - documentation,
        control files, etc.
        \begin{myitemize}
            \item The entire directory tree is copied if you copy the
                suite with cylc or gcylc.
            \item Run time task access via 
                \lstinline=$CYLC_SUITE_DEF_PATH= 
                (Section~\ref{TaskExecutionEnvironment}).
            \item Holding everything in one place makes proper suite
                revision control possible.
        \end{myitemize}
\end{myitemize}
An imaginary example:
\lstset{language=transcript}
\begin{lstlisting}
/path/to/my/suite   # suite definition directory
    suite.rc           # THE SUITE CONFIGURATION FILE
    bin/               # scripts and executables used by tasks
        foo.sh
        bar.sh
        ...
    # (OPTIONAL) any other suite-related files, for example:
    inc/               # suite.rc include-files
        nwp-tasks.rc
        globals.rc
        ...
    doc/               # documentation
    control/           # control files
    ancil/             # ancillary files
    ...
\end{lstlisting}

\subsection{Suite.rc Overview}
\label{SuiteRCFile}

Cylc suite.rc files are parsed directly into a nested data structure
that mirrors the file section nesting. This makes it very easy to
add new configuration items to cylc. The underlying file structure
is based on ConfigObj
(http://www.voidspace.org.uk/python/configobj.html), slightly modified
for cylc.

\subsubsection{Syntax}

\begin{myitemize}
    \item {\bf All entries} are of the form \lstinline@item = value@ 
        (some values require post-validation parsing).
    \item {\bf Comments} follow a hash character (\#) to the end of the line.
    \item {\bf List Values} are comma separated.
    \item {\bf Strings} must be quoted if they contain commas 
        (which indicate list values).
    \item {\bf Multiline Strings} must be triple-quoted.
    \item {\bf Boolean Values} are written as True or False (capitalized).
    \item {\bf White Space} is ignored; indentation can be used for clarity.
    \item {\bf Continuation Lines} follow a trailing backslash.
    \item {\bf [Section Headings]} are enclosed in square brackets.
    \item {\bf [[Subsection Nesting]]} is indicated by the number of
        square brackets.\footnote{Sections are closed by the next
        section heading, so items within a section must be
        defined before any subsequent subsection headings.}
    \item {\bf Duplicate Items} are illegal, except in
        \lstinline=environment= and \lstinline=directives=
        sections.\footnote{The exceptions were designed to allow 
        tasks to override environment variables defined in include-files
        that could be included in multiple tasks, to assist in factoring
        out common task configuration. However, namespace inheritance
        now provides a better way to do this in most cases.}
    \item {\bf Include-files} can be used, and may be multiply-included
        and nested. Inclusion boundaries are arbitrary (they can span
        sections).  Include-file paths should be specified portably
        relative to the suite definition directory, e.g.:
\lstset{language=suiterc}
\begin{lstlisting}
# include the file $CYLC_SUITE_DEF_PATH/inc/foo.rc:
%include inc/foo.rc
\end{lstlisting}
    \end{myitemize}

The following pseudo-listing illustrates suite.rc syntax:
\lstset{language=suiterc}
\begin{lstlisting}
# a full line comment
an item = value # a trailing comment
a boolean item = True # or False
one string item = the quick brown fox # string quotes optional ...
two string item = "the quick, brown fox" # ... unless internal commas
a multiline string item = """the quick brown fox
jumped over the lazy dog""" # triple quoted
a list item = foo, bar, baz   # comma separated
a list item with continuation = a, b, c, \
                                d, e, f
[section]
    item = value
%include inc/vars/foo.inc  # include file
    [[subsection]]
        item = value
        [[[subsubsection]]]
            item = value
[another section]
    [[another subsection]]
        # ...
    # ...
# ...
\end{lstlisting}

\paragraph{Syntax Highlighting In Vim}
\label{SyntaxHighlighting}

\lstset{language=transcript}
Cylc comes with a syntax file to configure suite.rc syntax
highlighting and section folding in the {\em vim} editor, as shown in
Figure~\ref{fig-cylc-vim}. To use this, copy 
\lstinline=$CYLC_DIR/conf/cylc.vim= to your
\lstinline=$HOME/.vim/syntax/= directory and make some minor
modifications, as described in the syntax file, to your
\lstinline=$HOME/.vimrc= file.

\subsubsection{Gross File Structure}

Cylc suite.rc files consist of a suite title and description followed by
configuration items grouped under several top level section headings:

\begin{myitemize}
    \item {\bf [cylc] } - {\em non task-related suite configuration}
        \begin{myitemize}
            \item suite logging directories, simulation mode, UTC mode, etc.
        \end{myitemize}
    \item {\bf [scheduling] } - {\em determines when tasks are ready to run}
        \begin{myitemize}
            \item tasks with special behaviour, e.g. clock-triggered tasks
            \item the dependency graph, defines relationships between tasks
        \end{myitemize}
    \item {\bf [runtime] } - {\em determines how, where, and what to
        execute when tasks are ready}
        \begin{myitemize}
            \item command scripting, environment, job submission, remote
                hosting, etc.
            \item suite-wide defaults in the {\em root} namespace
            \item a nested family hierarchy with common properties
                inherited by related tasks
        \end{myitemize}
    \item {\bf [visualization] } - suite graphing and the graph-based control GUI.
\end{myitemize}


\subsubsection{Validation}
\label{Validation}

Cylc suite.rc files are automatically validated against a specification
file that defines all legal entries, values, options, and defaults 
(\lstinline=$CYLC_DIR/conf/suiterc.spec=). This detects any formatting
errors, typographic errors, illegal items and illegal values prior to 
run time. Some values are complex strings that require further parsing
by cylc to determine their correctness (this is also done during
validation). All legal entries are documented in the {\em Suite.rc
Reference} (Appendix~\ref{SuiteRCReference}).

The validator reports the line numbers of detected errors. Here's an 
example showing a subsection heading with a missing right bracket.
\begin{lstlisting}
% cylc validate foo.bar
Parsing Suite Config File
ERROR: [[special tasks]
NestingError('Cannot compute the section depth at line 19.',)
_validate foo.bar failed:  1
\end{lstlisting}

If the suite.rc file contains include-files use \lstinline=cylc inline=
(or the gcylc right-click `Edit' inline option) to view an inlined copy
with correct line numbers (you can also edit suites in a temporarily
inlined state using \lstinline=cylc edit --inline=).

\subsubsection{String Interpolation}

Configuration items beginning with an underscore are ignored by cylc, but may be
used with the string interpolation mechanism provided by the ConfigObj file format. 
An example:
\lstset{language=suiterc}
\begin{lstlisting}
# SUITE.RC
_greeting = HELLO
[runtime]
    [[root]]
        command scripting = "echo %(_greeting)s from $CYLC_TASK_ID; sleep 10; echo BYE"
\end{lstlisting}
\lstset{language=transcript}
Then, assuming this suite is registered as \lstinline=foo=,
\begin{lstlisting}
% cylc get-config foo runtime root 'command scripting'
['echo HELLO from $CYLC_TASK_ID; sleep 10; echo BYE']
\end{lstlisting}

\subsection{Scheduling - Dependency Graphs}
\label{DependencyGraphs}
 
\lstset{language=suiterc}
The \lstinline=[scheduling]= section of a suite.rc file defines the
relationships
between tasks in a suite - the information that allows cylc to determine
when tasks are ready to run. The most important component of this is the
suite dependency graph. Cylc graph notation makes clear textual graph
representations that are more concise than the real thing because
sections of the graph that repeat at different hours of
the day only have to be defined once. Here's an example showing a 
simple task tree with dependencies that vary at certain hours:
\lstset{language=suiterc}
\begin{lstlisting}
[scheduling]
    [[dependencies]]
        [[[0,6,12,18]]] # validity (hours of the day)
            graph = """
A => B & C   # B and C trigger off A
A[T-6] => A  # Model A restart trigger
                    """
        [[[6,18]]] # hours
            graph = "C => X"
\end{lstlisting}
\lstset{language=transcript}
Figure~\ref{fig-dep-eg-1} shows the complete suite.rc listing alongside
the suite graph, plotted with,
\begin{lstlisting}
% cylc graph SDepG 2011052200 2011052206
# (or use right-click Graph in gcylc)
\end{lstlisting}
This is actually a complete, valid, runnable suite (it will use default 
runtime properties such as dummy command scripting, because no runtime 
properties are explicitly defined, and you'll need to trigger task A
manually to get the suite started because A[T] depends on A[T-6] and at
start-up there is no previous cycle to satisfy that dependence - how to
handle this properly is described in 
{\em Satisfying Intercycle Dependencies At Start-Up}
(Section~\ref{SatisfyingIntercycleDependenceAtStartUp}) and in the
{\em Quick Start Guide} (Section~\ref{QuickStartGuide}).

\begin{figure}
\begin{minipage}[b]{0.5\textwidth}
\lstset{language=suiterc}
\begin{lstlisting}
# SUITE.RC
title = "Dependency Graph Example"
[scheduling]
    [[dependencies]]
        [[[0,6,12,18]]] # validity (hours)
            graph = """
A => B & C   # B and C trigger off A
A[T-6] => A  # Model A restart trigger
                    """
        [[[6,18]]] # hours
            graph = "C => X"
[visualization]
    [[node attributes]]
        X = "color=red"
\end{lstlisting}
\lstset{language=transcript}
\end{minipage}
\hfill
\begin{minipage}[b]{0.5\textwidth}
    \begin{center}
        \includegraphics[width=\textwidth]{screenshots/dep-eg-1.png}
    \end{center}
\end{minipage}
\caption[Example Suite SDepG]{\scriptsize Example Suite SDepG}
\label{fig-dep-eg-1}
\end{figure}

\subsubsection{Graph String Syntax}

Multiline graph strings may contain:
\begin{myitemize}
    \item {\bf blank lines}
    \item {\bf arbitrary white space}
    \item {\bf task names}
    \item {\bf internal comments:} following the \# character
    \item {\bf cycle time offsets:} \lstinline=A[T-6]=
    \item {\bf success triggers:} \lstinline@foo => bar@
    \item {\bf failure triggers:} \lstinline@foo:fail => bar@
    \item {\bf suicide triggers:} \lstinline@foo => !bar@
    \item {\bf explicit output triggers:} \lstinline@foo:out1 => bar@
    \item {\bf conditional triggers:} \lstinline@ (A | B) & C => D@ 
\lstset{language=Python}
    \item {\bf trigger-generators:} 
        \lstinline@Python:list( "m"+str(i)+"=> p"+str(i) for i in range(1,7))@
\lstset{language=transcript}
\end{myitemize}

\subsubsection{Interpreting Graph Strings}

Suite dependency graphs can be broken down into pairs in which the left
side (which may be a single task or family, or several that are
conditionally related) defines a trigger for the task or family on the
right. For instance the ``word graph'' {\em C triggers off B which
triggers off A} can be deconstructed into pairs {\em C triggers off B}
and {\em B triggers off A}.

In the case of cycling tasks, the triggers defined by a graph string are 
valid for cycle times matching the list of hours specified for the
graph section. For example this graph,
\lstset{language=suiterc}
\begin{lstlisting}
# SUITE.RC
[scheduling]
    [[dependencies]]
        [[[0,12]]]
            graph = "A => B"
\end{lstlisting}
\lstset{language=transcript}
implies that B triggers off A for cycle times in which the hour matches
$0$ or $12$.

To define intercycle dependencies, attach an offset indicator to the
left side of a pair:
\lstset{language=suiterc}
\begin{lstlisting}
# SUITE.RC
[scheduling]
    [[dependencies]]
        [[[0,12]]]
            graph = "A[T-12] => B"
\end{lstlisting}
\lstset{language=transcript}
This means B[T] triggers off A[T-12] for cycle times T with hours
matching $0$ or $12$. Note that {\em T must be left implicit unless
there is a cycle time offset} (this helps to keep graphs clean and
concise because the majority of tasks in a typical suite will only
depend on others with the same cycle time) and that {\em cycle time
offsets can only appear on the left} (because each pair defines a
trigger for the right task at cycle time T).

Now, having explained that dependency graphs are interpreted pairwise,
you can optionally chain pairs together to ``follow a path'' through the
graph. So this,
\lstset{language=suiterc}
\begin{lstlisting}
# SUITE.RC
    graph = """A => B  # B triggers off A
               B => C  # C triggers off B"""
\end{lstlisting}
is equivalent to this:
\begin{lstlisting}
# SUITE.RC
    graph = "A => B => C"
\end{lstlisting}

Cycle time offsets, if they appear in a chain of triggers, must be
leftmost (because, as explained previously they can't appear on the
right of any pair). So this is legal:
\begin{lstlisting}
# SUITE.RC
    graph = "A[T-6] => B => C"  # OK
\end{lstlisting}
but this isn't:
\begin{lstlisting}
# SUITE.RC
    graph = "A => B[T-6] => C"  # ERROR!
\end{lstlisting}
The trigger \lstinline@A => B[T-6]@ does not make sense in any case -
if this kind of relationship seems necessary it probably means that B
should be ``reassigned'' to the next cycle (keep in mind that cycle
time is really just a label used to define the relatonships between
tasks).

{\em Each trigger in the graph must be unique} but {\em the same task
can appear in multiple pairs or chains}. Separately defined triggers
for the same task have an AND relationship. So this:
\begin{lstlisting}
# SUITE.RC
    graph = """A => X  # X triggers off A
               B => X  # X also triggers off B"""
\end{lstlisting}
is equivalent to this:
\begin{lstlisting}
# SUITE.RC
    graph = "A & B => X"  # X triggers off A AND B
\end{lstlisting}

In summary, the branching tree structure of a dependency graph can 
be partitioned into lines (in the suite.rc graph string) of pairs
or chains, in any way you like, with liberal use of internal white space
and comments to make the graph structure as clear as possible.

\begin{lstlisting}
# SUITE.RC
# B triggers if A succeeds, then C and D trigger if B succeeds:
    graph = "A => B => C & D"
# which is equivalent to this:
    graph = """A => B => C
               B => D"""
# and to this:
    graph = """A => B => D
               B => C"""
# and to this:
    graph = """A => B
               B => C
               B => D"""
# and it can even be written like this:
    graph = """A => B # blank line follows:

               B => C # comment ...
               B => D"""
\end{lstlisting}
\lstset{language=transcript}

\subsubsection{Graph Types (VALIDITY)}

A suite definition can contain multiple graph strings that are combined
to generate the final graph. There are different graph VALIDITY section
headings (the heading of the suite.rc section that encloses the graph
string) for cycling, one-off asynchronous, and repeating asynchronous
tasks. Additionally, there may be multiple graph strings (under different
VALIDITY sections) for cycling tasks, for tasks with different
dependencies at different cycle times.

{\em The small suite definitions used in the following subsections can
be found in the central database registered under the {\em validity}
group name.} 

\paragraph{One-off Asynchronous Tasks}

Figure~\ref{fig-test1} shows a small suite of one-off asynchronous
tasks; these have no associated cycle time and don't spawn
successors (once they're all finished the suite just exits).  The
integer $1$ attached to each graph node is just an arbitrary label, akin
to the task cycle time in cycling tasks; it increments when a repeating
asynchronous task (below) spawns.
\begin{figure}
\begin{minipage}[b]{0.5\textwidth}
\lstset{language=suiterc}
\begin{lstlisting}
# SUITE.RC
title = some one-off asynchronous tasks
[scheduling]
    [[dependencies]]
        graph = "foo => bar & baz => waz"
\end{lstlisting}
\lstset{language=transcript}
\end{minipage}
\hfill
\begin{minipage}[b]{0.5\textwidth}
    \begin{center}
        \includegraphics[width=0.25\textwidth]{screenshots/test1.png}
    \end{center}
\end{minipage}
\caption[One-off Asynchronous Tasks]{\scriptsize One-off Asynchronous Tasks.}
\label{fig-test1}
\end{figure}

\paragraph{Cycling Tasks}

Valid cycle times for cycling tasks are defined by the graph VALIDITY
section headings - lists of hours in the day - for the graph strings
in which the tasks appear. Figure~\ref{fig-test2} shows a small suite of
cycling tasks.
\begin{figure}
\begin{minipage}[b]{0.5\textwidth}
\lstset{language=suiterc}
\begin{lstlisting}
# SUITE.RC
title = some one-off asynchronous tasks
# (no dependence between cycles here)
[scheduling]
    [[dependencies]]
        [[[0,12]]]
            graph = "foo => bar & baz => waz"
\end{lstlisting}
\lstset{language=transcript}
\end{minipage}
\hfill
\begin{minipage}[b]{0.5\textwidth}
    \begin{center}
        \includegraphics[width=\textwidth]{screenshots/test2.png}
    \end{center}
\end{minipage}
\caption[Cycling Tasks]{\scriptsize Cycling Tasks.}
\label{fig-test2}
\end{figure}

\paragraph{Combined Graphs}

Cycling tasks can be made to wait on one-off asynchronous tasks, as
shown in Figure~\ref{fig-test4}. Alternatively, they can be made to wait 
on one-off {\em synchronous} start-up tasks, which have an associated cycle
time event though they are non-cycling tasks - see Figure~\ref{fig-test5}.

\subparagraph{Synchronous Start-up vs One-off Asynchronous Tasks}

One-off synchronous start-up tasks run only when a cycling suite is
{\em cold-started} and they are often associated with subsequent one-off
{\em cold-start tasks} used to bootstrap a cycling suite into existence. 

The distinction between cold- and warm-start is only meaningful for
cycling tasks, and one-off asynchronous tasks may be best used in
constructing entirely non-cycling suites.

However, one-off asynchronous tasks can precede cycling tasks in the
same suite, as shown above. It seems likely that, if used in this way,
they will be intended as start-up tasks - so currently {\em one-off
asynchronous tasks only run in a cold-start}.

\begin{figure}
\begin{minipage}[b]{0.5\textwidth}
\lstset{language=suiterc}
\begin{lstlisting}
# SUITE.RC
title = one-off async and cycling tasks
# (with dependence between cycles too)
[scheduling]
    [[dependencies]]
        graph = "prep1 => prep2"
        [[[0,12]]]
            graph = """
    prep2 => foo => bar & baz => waz
    foo[T-12] => foo
                    """
\end{lstlisting}
\lstset{language=transcript}
\end{minipage}
\hfill
\begin{minipage}[b]{0.5\textwidth}
    \begin{center}
        \includegraphics[width=\textwidth]{screenshots/test4.png}
    \end{center}
\end{minipage}
\caption[One-off Asynchronous and Cycling Tasks]{\scriptsize One-off
asynchronous and cycling tasks in the same suite.}
\label{fig-test4}
\end{figure}

\begin{figure}
\begin{minipage}[b]{0.5\textwidth}
\lstset{language=suiterc}
\begin{lstlisting}
# SUITE.RC
title = one-off start-up and cycling tasks
# (with dependence between cycles too)
[scheduling]
    [[special tasks]]
        start-up = prep1, prep2
    [[dependencies]]
        [[[0,12]]]
            graph = """
    prep1 => prep2 => foo => bar & baz => waz
    foo[T-12] => foo
                    """
\end{lstlisting}
\lstset{language=transcript}
\end{minipage}
\hfill
\begin{minipage}[b]{0.5\textwidth}
    \begin{center}
        \includegraphics[width=\textwidth]{screenshots/test5.png}
    \end{center}
\end{minipage}
\caption[One-off Synchronous and Cycling Tasks]{\scriptsize One-off
synchronous and cycling tasks in the same suite.}
\label{fig-test5}
\end{figure}

\paragraph{Repeating Asynchronous Tasks}

Repeating asynchronous tasks can be used to process satellite data that 
arrives asynchronously - i.e.\ at irregular time intervals. Each new dataset 
must have a unique ``asynchronous ID'' - if it doesn't already have such an 
ID you could use some string representation of the data arrival time. 
The graph VALIDITY section heading must contain ``ASYNCID:'' followed by 
a regular expression designed to match the actual IDs.  Addtionally,
one task in the suite must be designated the ``daemon'' - it waits
indefinitely on incoming data and reports each new dataset and its ID
back to the suite by means of a special output message. When the task 
proxy receives that message it dynamically registers a new output
(containing the asynchronous ID) that downstream tasks can then trigger
off. The downstream tasks likewise have prerequisites containing the
ID pattern (because they trigger off the aforementioned outputs) and
when these get satisfied the actual ID is substituted into their own 
registered outputs. Additionally, each asynchronous task proxy passes
the ID to its task execution environment as
\lstinline=$ASYNCID= to allow identification of the correct dataset by 
task scripts.
In this way the whole tree of tasks becomes dedicated to processing the
new dataset, but if the data arrives quickly successive datasets can
be processed in parallel. As Figure~\ref{fig-test3} shows, a repeating
asynchronous suite currently plots just like a one-off asynchronous
suite.  But at run time the daemon task stays put, while
the others continually spawn successors to wait for new datasets to
come in. The {\em asynchronous.repeating} example suite demonstrates how 
to do this in a real suite.
\begin{figure}
\begin{minipage}[b]{0.5\textwidth}
\lstset{language=suiterc}
\begin{lstlisting}
# SUITE.RC
title = a suite of repeating asynchronous tasks
# for processing real time satellite datasets
[scheduling]
    [[dependencies]]
        [[[ASYNCID:satX-\d{6}]]]
            # match datasets satX-1424433 (e.g.)
            graph = "watcher:a => foo:a & bar:a => baz"
            daemon = watcher
[runtime]
    [[watcher]]
        [[[outputs]]]
            a = "New dataset $(ASYNCID) ready for processing"
    [[foo,bar]]
        [[[outputs]]]
            a = "Products generated from dataset $(ASYNCID)"
\end{lstlisting}
\lstset{language=transcript}
\end{minipage}
\hfill
\begin{minipage}[b]{0.5\textwidth}
    \begin{center}
        \includegraphics[width=0.45\textwidth]{screenshots/test6.png}
    \end{center}
\end{minipage}
\caption[Repeating Asynchronous Tasks]{\scriptsize Repeating Asynchronous Tasks.}
\label{fig-test3}
\end{figure}

\subsubsection{Trigger Types}

\lstset{language=suiterc}

\paragraph{Success Triggers}

To trigger off a task finishing successfully:
\begin{lstlisting}
# SUITE.RC
# B triggers if A SUCCEEDS:
    graph = A => B
\end{lstlisting}

\paragraph{Failure Triggers}

To trigger off a task that fails:
\begin{lstlisting}
# SUITE.RC
# B triggers if A FAILS:
    graph = A:fail => B
\end{lstlisting}

See {\em Suicide Triggers} (Section~\ref{SuicideTriggers}) for
how to handle task B if A does not fail.

To trigger off a task finishing (success or failure):
\begin{lstlisting}
# SUITE.RC
# B triggers if A either SUCCEEDS or FAILS:
    graph = A | A:fail => B
\end{lstlisting}

\paragraph{Internal Triggers}

{\em Only required if you need to trigger before the upstream task finishes.}
\begin{lstlisting}
# SUITE.RC
[scheduling]
    [[dependencies]]
        [[[6,18]]]
            # B triggers off specific OUTPUT "out1" of task A:
            graph = A:out1 => B
[runtime]
    [[A]]
        [[[outputs]]]
            out1 = "NWP products uploaded for $(CYLC_TASK_CYCLE_TIME)"
\end{lstlisting}
Task A must emit this message when the actual output has been completed - 
see {\em Reporting Internal Outputs Completed}, Section~\ref{RIOC}.

\paragraph{Intercycle Triggers}
\label{IntercycleTriggers}

Typically most tasks in a suite will trigger off other cotemporal tasks (same cycle
time) but some may depend on tasks with earlier cycle times. This
notably applies to warm-cycled forecast models, which depend
on their own previous instances (see below); but other kinds of
intercycle are possible too.\footnote{In NWP forecast
analysis suites parts of the observation processing and data
assimilation subsystem will typically also depend on model background
fields generated by the previous forecast.} Here's how to express this
kind of relationship in cylc:
\begin{lstlisting}
# SUITE.RC
[dependencies]
    [[0,6,12,18]]
        # B triggers off A in the previous cycle
        graph = "A[T-6] => B"
\end{lstlisting}
This notation can be used with explicit outputs too:
\begin{lstlisting}
# SUITE.RC
    # B triggers if A in the previous cycle fails:
    graph = "A[T-6]:fail => B"
\end{lstlisting}
 
\paragraph{Conditional Triggers}

AND operators (\lstinline=&=) can appear on both sides of an arrow. They
provide a concise alternative to defining multiple triggers separately:
\begin{lstlisting}
# SUITE.RC
# 1/ this:
    graph = "A & B => C"
# is equivalent to:
    graph = """A => C
               B => C"""
# 2/ this:
    graph = "A => B & C"
# is equivalent to:
    graph = """A => B
               A => C"""
# 3/ and this:
    graph = "A & B => C & D"
# is equivalent to this:
    graph = """A => C
               B => C
               A => D
               B => D"""
\end{lstlisting}

OR operators (\lstinline=|=), for truly conditional triggers,
can only appear on the left,\footnote{An OR
operator on the right doesn't make much sense: if ``B or C'' triggers
off A, what exactly should cylc do when A finishes?}
\begin{lstlisting}
# SUITE.RC
# C triggers when either A or B finishes:
    graph = "A | B => C"
\end{lstlisting}

Forecasting suites typically have quite simple requirements for
conditional triggers, but should you need it you can use any valid
parenthesised conditional expression, e.g.:
\begin{lstlisting}
# SUITE.RC
    graph = """
        # D triggers if A or (both B and C) succeed:
        A | B & C => D
        # Z triggers if both (W or X) and Y succeed.
        (W | X) & Y => Z
            """ 
\end{lstlisting}

Condititional triggers are plotted with open arrow heads.

\paragraph{Suicide Triggers}
\label{SuicideTriggers}

Suicide triggers take tasks out of the suite. This can be used for
automated failure recovery. The suite.rc listing and accompanying 
graph in Figure~\ref{fig-suicide} show how to define a chain of failure
recovery tasks
that trigger if they're needed but otherwise remove themselves from the
suite (you can run the {\em AutoRecover.async} example suite to see how
this works).  The dashed graph edges ending in solid dots indicate
suicide triggers, and the open arrowheads indicate conditional triggers
as usual.

\begin{figure}
\begin{minipage}[b]{0.5\textwidth}
\lstset{language=suiterc}
\begin{lstlisting}
# SUITE.RC
title = asynchronous automated recovery
description = """
Model task failure triggers diagnosis 
and recovery tasks, which take themselves
out of the suite if model succeeds. Model
post processing triggers off model OR 
recovery tasks.
              """
[scheduling]
    [[dependencies]]
        graph = """
pre => model
model:fail => diagnose => recover
model => !diagnose & !recover
model | recover => post
                """
[runtime]
    [[model]]
        # UNCOMMENT TO TEST FAILURE:
        # command scripting = false
\end{lstlisting}
\lstset{language=transcript}
\end{minipage}
\hfill
\begin{minipage}[b]{0.5\textwidth}
    \begin{center}
        \includegraphics[width=0.5\textwidth]{screenshots/suicide.png} 
    \end{center}
\end{minipage}
\caption[Automated failure recovery via suicide triggers] {\scriptsize
Automated failure recovery via suicide triggers.}
\label{fig-suicide} 
\end{figure}

\paragraph{Family Triggers}

Family names defined by the namespace inheritance hierarchy 
(see Section~\ref{NIORP}) can be used in the suite graph as shorthand
for the effective dependencies on member tasks. Member dependencies are
substituted in such a way that downstream tasks will not trigger until
all family members have either succeeded or failed. For example the
graph in this suite:
\begin{lstlisting}
# SUITE.RC
[scheduling]
    [[dependencies]]
        graph = "foo => fam => bar"
[runtime]
    [[fam]]  # a family (because others inherit from it)
    [[a,b]]  # family members (inherit namespace fam)
        inherit = fam
\end{lstlisting}
is equivalent to this:
\begin{lstlisting}
# SUITE.RC
[scheduling]
    [[dependencies]]
        graph = "foo => a & b => bar"
\end{lstlisting}
And the graph in this suite:
\begin{lstlisting}
# SUITE.RC
[scheduling]
    [[dependencies]]
        graph = "fam:fail => bar"
[runtime]
    [[fam]]
    [[a,b]]
        inherit = fam
\end{lstlisting}
is equivalent to this:
\begin{lstlisting}
# SUITE.RC
[scheduling]
    [[dependencies]]
        graph = "( a:fail|b:fail ) & ( a|a:fail ) & ( b|b:fail ) => bar"
\end{lstlisting}

{\em Task family triggers should be used sparingly} as a convenient
simplification for groups of tasks that would naturally trigger at the
same time anyway (e.g.\ forecast ensembles and multiple tasks for
processing different types of observations that are all made available
at the same time) and whose outputs would all be put to use at a similar
time too. Otherwise family triggers will unnecessarily
constrain cylc's ability to achieve maximum functional parallelism,
because all family members have to finish before downstream processing
can continue.\footnote{Actually downstream tasks can also trigger off
individual family members, rather than off the family as a whole, but
extensive use of this would probably defeat the purpose of using a
family in the first place.}

\subsubsection{Satisfying Intercycle Dependence At Start-Up}
\label{SatisfyingIntercycleDependenceAtStartUp}

In suites with intercycle dependence some kind of bootsrapping
process is required to get the suite going initially. In the example 
shown in {\em Intercycle Triggers} (Section~\ref{IntercycleTriggers}),
for instance, in the very first cycle there is no previous instance of 
task A to satisfy B's prerequisites. 
 
\paragraph{Cold-Start Tasks}

A {\em cold-start} task is a special one-off task used to satisfy the
initial previous-cycle dependence of another cotemporal task. In effect, 
the cold-start task masquerades as the previous-cycle trigger of its
associated cycling task. 

A cold-start task may invoke real processing, probably to generate the
files that are normally produced by the associated cycling task; or it
could be a dummy task that represents some external spinup process,
presumably resulting in the same files, that has to be completed before
the suite is started. In the latter case the cold-start task can 
just report itself successfully completed after checking that the 
required files are present.

This kind of relationship can easily be expressed with a conditional
trigger:
\begin{lstlisting}
# SUITE.RC
[scheduling]
    [[special tasks]]
        cold-start = ColdFoo
    [[dependencies]]
        [[[0,6,12,18]]]
            graph = "ColdFoo | Bar[T-6] => Foo"
\end{lstlisting}
i.e.\ Foo[T] can trigger off {\em either} Bar[T-6] {\em or} ColdFoo[T].
At start-up ColdFoo will do the job, and thereafter Bar[T-6] will do it.

{\em Cold-start tasks can also be inserted into the suite at run time 
to cold-start just their associated cycling tasks, if a problem of 
some kind prevents continued normal cycling}.

\paragraph{Warm-Starting A Suite}

Cold-start tasks have to be declared as such in the suite.rc ``special
tasks'' section so that cylc knows they are one-off (non-spawning) tasks,
but also because they play a critical role in suite warm-starts. 
A suite that has previously been running and was then shut down 
can be warm-started at a particular cycle time, an alternative to {\em
restarting} from a previous state (although restarting is preferred
because a warm start is likely to involve re-running some tasks).
A warm-start assumes the existence of a previous cycle (i.e.\ that any
files from the previous cycle required by the new cycle are in place
already) so cold-start tasks do not need to run {\em but} cylc itself
does not know the details of the previous cycle (it does in a restart,
but not in a warm-start) so it still has to solve the bootstrapping
problem to get the suite started. It does this by starting the suite
with designated cold start tasks in the {\em succeeded} state - in other
words finished cold start tasks stand in for the previous finished cycle,
rather than pretending to be a running previous cycle as they do in a
cold-start.

\subsubsection{Model Restart Dependencies}
\label{ModelRestartDependencies}

Warm cycled forecast models generate {\em restart files}, e.g.\ model
background fields, that are required to initialize the next forecast
(this is essentially the definition of ``warm cycling''). In fact
restart files will often be written for a whole series of subsequent
cycles in case the next cycle (or the next and the next-next, and so on)
cycle has to be omitted: 
\begin{lstlisting}
# SUITE.RC
[scheduling]
    [[special tasks]]
        sequential = A
    [[dependencies]]
        [[[0,6,12,18]]]
            # Model A cold-start and restart dependencies:
            graph = "ColdA | A[T-6] | A[T-12] | A[T-18] | A[T-24] => A"
\end{lstlisting}
In other words, task A can trigger off a cotemporal cold-start task,
{\em or} off its own previous instance, {\em or} off the instance before
that, and so on.
Restart dependencies are unusual because although A {\em could}
trigger off A[T-12] we don't actually want it to do so unless A[T-6]
fails and can't be fixed. {\em This is why Task A, above, is declared to be
`sequential'}.\footnote{A warm cycling model that only writes out
one set of restart files, for the very next cycle, does not need to be
declared sequential because this early triggering problem cannot arise.}
Sequential tasks do not spawn a successor until they have
succeeded (by default, tasks spawn as soon as they start running in
order to get maximum functional parallelism in a suite) which
means that A[T+6] will not be waiting around to trigger off an older
predecessor while A[T] is still running. If A[T] fails though, the
operator can force it, on removal, to spawn A[T+6], whose restart
dependencies will then automatically be satisfied by the older instance,
A[T-6]. 

Forcing a model to run sequentially means, of course, that its restart
dependencies cannot be violated anyway, so we might just ignore them.
This is certainly an option, but it should be noted that there are some 
benefits to having your suite reflect all of the real dependencies
between the tasks that it is managing, particularly for complex
multi-model operational suites in which the suite operator might not be
an expert on the models. Consider such a suite in which a failure in a
driving model (e.g.\ weather) precludes running one or more 
cycles of the downstream models (sea state, storm surge, river flow,
\dots). If the real restart dependencies of each model are known to the
suite, the operator can just do a recursive purge to remove the subtree
of all tasks that can never run due to the failure, and then cold-start
the failed driving model after a gap (skipping as few cycles as possible
until the new cold-start input data are available). After
that the downstream models will kick off automatically so long as the 
gap is spanned by their respective restart files, because their
restart dependencies will automatically be satisfied by the older
pre-gap instances in the suite. Managing this kind of scenario manually
in a complex suite can be quite difficult.

Finally, if a warm cycled model is declared to have explicit restart
outputs, and is not declared to be sequential, and you define appropriate 
labeled restart outputs which {\em must contain the word `restart'},
then the task will spawn as soon its last restart output is
completed so that successives instances of the task will be able to
overlap (i.e.\ run in parallel) if the opportunity arises. Whether or
not this is worth the effort depends on your needs.
\begin{lstlisting}
# SUITE.RC
[scheduling]
    [[special tasks]]
        explicit restart outputs = A
    [[dependencies]]
        [[[0,6,12,18]]]
            graph = "ColdA | A[T-18]:res18 | A[T-12]:res12| A[T-6]:res6 => A"
[runtime]
    [[A]]
        [[[outputs]]]
            r6  = restart files completed for $(CYLC_TASK_CYCLE_TIME+6)
            r12 = restart files completed for $(CYLC_TASK_CYCLE_TIME+12)
            r18 = restart files completed for $(CYLC_TASK_CYCLE_TIME+18)
\end{lstlisting}


\subsection{Runtime - Namespaces}
\label{NIORP}

The \lstinline=[runtime]= section of a suite.rc file configures what to
execute, and where and how, when tasks are ready to run. The sections
immediately below \lstinline=[runtime]= are, for want of a better term,
{\em namespaces}\footnote{We use the term namespace in loose analogy with its
meaning in modern programming languages.  Possible future enhancements
to cylc, such as ability to import specific items from other namespaces
rather than just wholesale inheritance, may tighten the analogy.} that
define runtime properties for individual tasks and families of tasks.

Every namespace contains the same set of configuration items (see the
{\em Suite.rc Reference}, Appendix~\ref{SuiteRCReference}, for the
complete list of items).  Namespaces can inherit from other
namespaces, overriding inherited items as required; {\em this
allows configuration of related tasks without repetition}. 
A namespace represents a family if other namespaces inherit from it. 

A namespace that does not explicitly inherit from another automatically
inherits from the {\em root} namespace (below). Namespaces thus form a
tree-like hierarchy of nested task families, rooted on the root
namespace, in which the leaves are the individual tasks of the suite. 

Nested families from the namespace inheritance hierarchy, even if 
they are not used as family triggers in the graph, can be expanded or
collapsed in suite graphs, and in the graph-based suite control GUI
(this will also be added to the text treeview control GUI in a future
release). See {\em Visualization} (Section~\ref{viso}) for more on this. 

The following listing of the {\em namespace.one} example suite (which
you can import from the central database if you like) illustrates basic
runtime property inheritance. How it works should be reasonable clear by
inspection; if not read on.

\lstset{language=suiterc}
\lstinputlisting{../examples/namespace/one/suite.rc}
\lstset{language=transcript}

\subsubsection{Namespace Names}

Namespace names may contain letters, digits, underscores, and hyphens.
They may not contain colons as that would preclude use of suite 
registration names in shell \lstinline=$PATH= variables. The 
`.' character is the suite registration hierarchy delimiter (which
separates suite registration groups and names, e.g.\
my\_suites.test.foo). 
{\em Task names should not be hardwired into task implementations}
because task and suite identity can be extracted portably from the
task execution environment supplied by cylc 
(Section~\ref{TaskExecutionEnvironment}) - then to rename a task, 
can just change its name in the suite.rc file.

\subsubsection{Root - Runtime Defaults}

The root namespace, at the base of the inheritance hierarchy,
provides default configuration for all tasks in the suite. 

Most root items are unset by default, but some values are set,
sufficient to allow simple test suites to be defined by dependency graph
alone - as can be seen from many of the examples 
that illustrate this User Guide (command scripting, for example,
defaults to printing a simple message, sleeping for ten seconds, and
then exiting). These default values are documented with each
configuration item in Appendix~\ref{SuiteRCReference}, and
Section~\ref{SuiteDefaults} shows them all in context. 
You can override them or provide your own defaults for other items by
explicitly configuring the root namespace. 

\subsubsection{Defining Multiple Namespaces At Once}
\label{MultiTaskDef}

Groups of similar tasks or families that differ only in a few
configuration details can be defined in single namespace sections
by putting a list of names, or a Python expression that generates a list
of names, in the section heading. Potential applications include groups
of similar obs processing tasks, and forecasting ensembles.

Any occurrence of \lstinline=$(TASK)=, in any configuration items in 
the namespace, will be replaced with the actual name of the task or
family prior to run time. 

{\em The two annotated {\em multidef} example suites use this feature.}

\subsubsection{Task Execution Environment}
\label{TaskExecutionEnvironment}

The task execution environment contains suite and task identity
variables provided by cylc, and user-defined environment variables.
The environment is explicitly exported (by the task job script) prior to
executing task command scripting (see {\em Task Job Submission},
Section~\ref{TaskJobSubmission}).  

Suite and task identity are exported first, so that user-defined
variables can refer to them. Order of definition is preserved throughout 
so that variable assignment expressions can safely refer to previously 
defined variables.

Additionally, access to cylc itself is configured prior to the user-defined 
environment, so that variable assignment expressions can make use of 
cylc utility commands: 
\lstset{language=suiterc}
\begin{lstlisting}
# SUITE.RC
[runtime]
    [[foo]]
        [[[environment]]]
            REFERENCE_TIME = $( cylc util cycletime --add=6 )
\end{lstlisting}

\paragraph{User-defined Environment Variables}

The user-defined environment is the sum of a task's inherited
\lstinline=[[[environment]]]= sections.
\begin{lstlisting}
# SUITE.RC
[runtime]
    [[root]]
        [[[environment]]]
            COLOR = red
            SHAPE = circle
    [[foo]]
        [[[environment]]]
            COLOR = blue  # this overrides the root environment
\end{lstlisting}         
This results in a task {\em foo} with \lstinline@COLOR=blue@
and \lstinline@SHAPE=circle@ in its environment.

\paragraph{Suite And Task Identity Variables}

The task identify variables provided to tasks by cylc are:
\lstset{language=bash}
\begin{lstlisting}
$CYLC_TASK_ID                    # X%2011051118 (e.g.)
$CYLC_TASK_NAME                  # X
$CYLC_TASK_CYCLE_TIME            # 2011051118
$CYLC_TASK_NAMESPACE_HIERARCHY   # "root postproc X" (e.g.)
\end{lstlisting}

The suite identify variables provided to tasks by cylc are:

\begin{lstlisting}
$CYLC_SUITE_DEF_PATH   # $HOME/mysuites/baz (e.g.)
$CYLC_SUITE_REG_NAME   # foo.bar.baz (e.g.)
$CYLC_SUITE_REG_PATH   # foo/bar/baz
$CYLC_SUITE_HOST       # orca.niwa.co.nz (e.g.) 
$CYLC_SUITE_PORT       # 7766 (e.g.)
$CYLC_SUITE_OWNER      # oliverh (e.g.)
\end{lstlisting}
The variable \lstinline=$CYLC_SUITE_REG_PATH= is just 
\lstinline=$CYLC_SUITE_REG_NAME= (the name under which the suite
definition is registered in your suite database) translated into a
directory path. This can be used when configuring suite logging
directories and the like to put suite output in a directory tree that
reflects the suite registration hierarchy (as opposed to the namespace
hierarchy).
\lstset{language=transcript}

Some of the suite and task identity variables are also used
by cylc task messaging commands in order to automatically target the
right task proxy object in the right suite.

\paragraph{Environment Variable Evaluation}

Variables in the task execution environment are not evaluated in the
shell in which the suite is running prior to submitting the task. They
are written in unevaluated form to the job script that is submitted by
cylc to run the task (Section~\ref{JobScripts}) and are therefore
evaluated when the task begins executing under the task owner account 
on the task host. Thus \lstinline=$HOME=, for instance, evaluates at
run time to the home directory of task owner on the task host. 

\subsubsection{Remote Task Hosting}
\label{RunningTasksOnARemoteHost}

If a task declares an owner other than the suite owner and/or 
a host other than the suit host, e.g.:
\lstset{language=suiterc}
\begin{lstlisting}
# SUITE.RC
[runtime]
    [[foo]]
        [[[remote]]]
            host = orca.niwa.co.nz
            owner = bob
            cylc directory = /path/to/remote/cylc/installation/on/foo
            suite definition directory = /path/to/remote/suite/definition/on/foo
\end{lstlisting}
\lstset{language=transcript}
cylc will attempt to execute the task on the declared host, by the
configured job submission method, as the declared owner,
using passwordless ssh. 

\begin{myitemize}
    \item passwordless ssh must be configured between the suite owner on
        the suite host and the task owner on the remote host.
    \item cylc and Pyro must be installed on the remote host so that the
        remote task can communicate with the suite (but graphviz is not
        needed).
    \item the suite definition directory must be installed on the remote
        host, if the task needs access to scripts in the suite bin
        directory or to any other files stored there.
\end{myitemize}

A local task to run under another user account is treated as a remote task.

{\em You may not need this functionality if you have a cross-platform
resource manager, such as loadleveler, that allows you to submit a job
locally to run on the remote host}.

Remote host functionality, like other namespace properties, can 
be declared globally (in the root namespace) or per family, or per 
individual task. Use the global settings if all or most of your tasks
need to run on the same remote host.

The remote cylc directory is required to give remote tasks access 
to cylc commands; the remote suite directory gives them access to suite
files via \lstinline=$CYLC_SUITE_DEF_PATH= on the remote platform, and
to the suite bin directory via \lstinline=$PATH=. If a remote suite
definition directory is not given it will be assumed that the local
path should be used - but the local user's home directory (if present)
will be substituted in the file path for the literal \lstinline=$HOME=
in case the user's home directory path is different on the remote host.

Note that you can easily run the cylc example suites on a remote
host. For the example suites with task implementations that work
with ``real'' files, you'll have to run {\em all} tasks on the same
remote host so that they can access their common input and output
files. To distribute a suite across several hosts you must arrange
(using additional tasks) to transfer files between the hosts as required
to satisfy the real I/O dependencies.

\paragraph{Remote Log Directories}

The stdout and stderr from local tasks is directed into files in the
{\em job submission log directory} (specified in the suite.rc file) 
as explained in Section~\ref{WhitherStdoutAndStderr}. The same goes
for remotely hosted tasks, except that the task owner's home directory
is substituted as described above for the remote suite definition
directory. Remote log directories are created on the fly by cylc, 
during job submission, if they do not already exist.
 

\subsection{Visualization}
\label{viso}

This is the final major section in the suite.rc file. It is used to
configure suite graph plotting - principally graph node (task)
and edge (dependency arrow) style attributes. Tasks can be grouped for
the purpose of applying common style attributes. See the suite.rc
reference (Appendix~\ref{SuiteRCReference}) for details.

\subsubsection{Collapsible Task Families In Suite Graphing And GUIs}

\lstset{language=suiterc}
\begin{lstlisting}
# SUITE.RC
[visualization]
    # list namespace families to be shown in collapsed form
    collapsed families = family1, family2 
\end{lstlisting}
\lstset{language=transcript}

Nested families from the namespace inheritance hierarchy, even if 
they are not used as family triggers in the graph, can be expanded or
collapsed in suite graphs and by menu options in the graph-based suite control GUI (this
will also be added to the text treeview control GUI in a future
release). 

Family nodes in the graph-based suite control GUI are not currently 
color-coded in real time according to what state their members are in.
However, any ungraphed tasks, which includes the members of collapsed
families, are automatically plotted as rectangular nodes to the
right of the main graph if they are doing anything interesting
(submitted, running, or failed).

Note that family relationships can be defined purely for visualization
purposes - you can group tasks at root level in the inheritance
hierarchy prior to defining real properties at higher levels.

Figure~\ref{fig-namespaces} illustrates successive expansion of nested task
families in the {\em namespaces} example suite, which has the following
namespace hierarchy:
\begin{lstlisting}
% cylc list --tree cylc-x:y:z.namespaces
root              
 |-GEN            
 | |-OPS          
 | | |-aircraft   OPS aircraft obs processing
 | | |-atovs      OPS ATOVS obs processing
 | | `-atovs_post OPS ATOVS postprocessing
 | `-VAR          
 |   |-AnPF       runs VAR AnalysePF
 |   `-ConLS      runs VAR ConfigureLS
 |-baz            
 | |-bar1         Task bar1 of baz
 | `-bar2         Task bar2 of baz
 |-foo            No description provided
 `-prepobs        obs preprocessing
 \end{lstlisting}

\begin{figure}
\begin{minipage}[t]{0.3\textwidth}
    \begin{center}
        \includegraphics[width=\textwidth]{screenshots/inherit/inherit-2.png}
    \end{center}
\end{minipage}
\hfill
\begin{minipage}[t]{0.3\textwidth}
    \begin{center}
        \includegraphics[width=\textwidth]{screenshots/inherit/inherit-3.png}
    \end{center}
\end{minipage}
\hfill
\begin{minipage}[t]{0.3\textwidth}
    \begin{center}
        \includegraphics[width=\textwidth]{screenshots/inherit/inherit-4.png}
    \end{center}
\end{minipage}

\begin{minipage}[t]{0.3\textwidth}
    \begin{center}
        \includegraphics[width=\textwidth]{screenshots/inherit/inherit-5.png}
    \end{center}
\end{minipage}
\hfill
\begin{minipage}[t]{0.3\textwidth}
    \begin{center}
        \includegraphics[width=\textwidth]{screenshots/inherit/inherit-6.png}
    \end{center}
\end{minipage}
\hfill
\begin{minipage}[t]{0.3\textwidth}
    \begin{center}
        \includegraphics[width=\textwidth]{screenshots/inherit/inherit-7.png}
    \end{center}
\end{minipage}
\caption[{\em namespaces} example suite graphs]{\scriptsize Graphs of the {\em
namespaces} example suite showing various states of expansion of the
nested namespace family hierarchy, from all families collapsed (top
left) through to all expanded (bottom right). This can also be done by 
right-clicking on tasks in the graph-based suite control GUI.} \label{fig-namespaces}
\end{figure} 


%%\subsection{An Example}
%%
%%The following suite.rc file defines a default job
%%submission method (and an environment variable \lstinline=$COLOR=) for
%%all tasks; and a family called {\em obs} that contains the tasks {\em
%%land} and {\em ship}, each of which add to or override some of their
%%inherited properties.
%%\lstset{language=suiterc}
%%\lstinputlisting{../examples/ToDo/appendix/suite.rc}
%%\lstset{language=transcript}
%%You can use \lstinline=cylc get-config= to look at the final value 
%%of any item:
%%\begin{lstlisting}
%%% cylc db reg $CYLC_DIR/examples/appendix appx
%%% cylc get-config appx runtime land environment RUNNING_DIR
%%$HOME/running/$CYLC_TASK_NAME
%%\end{lstlisting}
%%
%%Using \lstinline=cylc jobscript= you can generate job scripts (used by cylc
%%to run tasks) showing the complete runtime configuration, resulting from 
%%namespace inheritance, for a task. The namespace hierarchy itself can be
%%printed with \lstinline=cylc list --tree=:
%%
%%\begin{lstlisting}
%%% cylc list -t appx
%%root      
%% |-obs    
%% | |-land land obs processing
%% | `-ship ship obs processing
%% |-prep   No description provided
%% `-prod   No description provided
%%\end{lstlisting}


\section{Task Implementation}
\label{TaskImplementation}

This section lays out the minimal requirements on external commands,
scripts, or executables invoked by cylc to carry out task processing.

\subsection{Most Tasks Require No Modification For Cylc}

Any existing command, script, or executable can function as a cylc task
(or rather, perform the external processing that
the task represents) if the following conditions are met:
\begin{myitemize}
    \item The task must not have internal/early outputs that others
        trigger off - otherwise the external task processing must be
        modified to report when those outputs are completed.
    \item The external process invoked by cylc must not detach and exit
        after spawning secondary processes to complete the job  -
        otherwise the secondary processes must be modified to
        report final success or failure to the suite.
    \item Any external processing must return zero exit status on 
        success, and non-zero on failure (this is normal practice
        anyway) to allow cylc's automatic error trapping to work.
\end{myitemize}

If these requirements are not met, see {\em Some Tasks Require
Modification For Cylc}, Section~\ref{TaskModsForCylc}. 

The following suite runs a couple of external scripts that are not
cylc-aware, but which meet the requirements above so that no special
treatment is required at all:
\lstset{language=suiterc}
\begin{lstlisting}
# SUITE.RC
[runtime]
    [[foo]]
        description = a task that runs foo.sh
        command scripting = foo.sh OPTIONS ARGUMENTS
    [[bar]]
        description = a task that runs bar.sh
        command scripting = """echo HELLO
                               bar.sh
                               echo BYE""" 
    [[baz]]
        description = a task that runs baz.sh and retries on failure
        command scripting = """echo attempt No.1
                               baz.sh""",
                            """echo attempt No.2  # only invoked if try No.1 fails
                               baz.sh --retry"""
\end{lstlisting}
\lstset{language=transcript}

\subsection{Suite.rc Inlined Tasks}

Simple tasks can be entirely implemented within the suite.rc file
because the task {\em command scripting} string can contain as many
lines of code as you like (or even a list of such strings, for retry 
on failure).

\subsection{Return Non-zero Exit Status On Error}

The requirement to abort with non-zero exit status on error
(which should be normal scripting practice in any case) 
allows the task job script to trap errors and send a 
\lstinline=cylc task failed= message to alert the suite. You can 
use \lstinline=set -e= to avoid writing explicit error checks for every
operation:

\lstset{language=bash}
\begin{lstlisting}
#!/bin/bash
set -e  # abort on error
mkdir /illegal/dir  # this error will abort the script with non-zero exit status
\end{lstlisting}

\subsection{Some Tasks Require Modification For Cylc}
\label{TaskModsForCylc}

\subsubsection{Voluntary Messaging}

You can, if you like, modify task scripts to send any explanatory or
progress messages to the suite as the task runs.  For example, a task
can send a priority critical message before aborting on error:
\lstset{language=bash}
\begin{lstlisting}
#!/bin/bash
set -e  # abort on error
if ! mkdir /illegal/dir; then
    # (use inline error checking to avoid triggering the above 'set -e')
    cylc task message -p CRITICAL "Failed to create directory /illegal/dir"
    exit 1 # now abort non-zero exit status to trigger the task failed message
fi
\end{lstlisting}

You can also use this syntax:
\lstset{language=bash}
\begin{lstlisting}
#!/bin/bash
set -e
mkdir /illegal/dir || {  # inline error checking using OR operator
    cylc task message -p CRITICAL "Failed to create directory /illegal/dir"
    exit 1
}
\end{lstlisting}

But not this:
\lstset{language=bash}
\begin{lstlisting}
#!/bin/bash
set -e
mkdir /illegal/dir  # aborted via 'set -e'
if [[ $? != 0 ]]; then  # so this will never be reached.
    cylc task message -p CRITICAL "Failed to create directory /illegal/dir"
    exit 1
fi
\end{lstlisting}

You can also send warning messages, or general information:
\lstset{language=bash}
\begin{lstlisting}
#!/bin/bash
# a warning message (this will be logged by the suite):
cylc task message -p WARNING "oops, something's fishy here"
# information (this will also be logged by the suite):
cylc task message "Hello from task foo"
\end{lstlisting}

This may be useful - any message received from a task is logged by cylc
- but it is not a requirement.  If error messages are not reported, for
instance, task failure will still be registered, and task stdout and
stderr logs can still be examined for evidence of what went wrong.

\subsubsection{Reporting Internal Outputs Completed}
\label{RIOC}

Tasks with internal outputs that allow downstream processing to
trigger before they are finished must report when those internal outputs
have been completed:
\lstset{language=bash}
\begin{lstlisting}
#!/bin/bash
# (task foo implementation)
# ...
# report an output completed:
cylc task message "foo products uploaded for $CYLC_TASK_CYCLE_TIME"
\end{lstlisting}
This must match one of the task's registered outputs:
\lstset{language=suiterc}
\begin{lstlisting}
# SUITE.RC
[scheduling]
    [[dependencies]]
        [[[6,18]]]
            graph = foo:output1 => bar
[runtime]
    [[foo]]
        [[[outputs]]]
            output1 = "foo products uploaded for $(CYLC_TASK_CYCLE_TIME)"
\end{lstlisting}
otherwise it will just be logged as a progress report or similar. {\em Note
the required round brackets in the suite.rc cycle time variable.}

\subsubsection{Tasks With Initiating Processes That Detach And Exit Early}
\label{DetachingTasks}

Tasks with initiating scripts or processes that spawn jobs internally
(e.g.\ to a batch queue scheduler or to another host) and then
detach and exit without seeing the resulting processing through 
must arrange for the spawned processing to send its own ``cylc
task succeeded'' or ``cylc task failed'' messages on completion -
because the cylc-generated job script (Section~\ref{JobScripts}) that
otherwise does automatic completion cannot know when the task is really
finished. 

For tasks in this category you must disable automatic completion
messaging:
\lstset{language=suiterc}
\begin{lstlisting}
# SUITE.RC
[runtime]
    [[root]]
        manual completion = True   # global setting
    [[foo]]
        manual completion = False  # task-specific setting
\end{lstlisting}

Reporting success or failure is just a matter of calling the cylc
messaging commands:
\lstset{language=bash}
\begin{lstlisting}
#!/bin/bash
# ...
if $SUCCESS; then
    # release my task lock and report success
    cylc task succeeded
    exit 0
else
    # release my task lock and report failed
    cylc task failed "Input file X not found"
    exit 1
fi
\end{lstlisting}

Bear in mind, however, that cylc messaging commands read environment
variables that identify the calling task and the target suite, so if
your job submission method does not automatically copy its parent
environment you must arrange for these variables, at the least, to be
propagated through to your spawned sub-jobs.

One way to handle this is to write a {\em task wrapper} that modifies a
copy of the detaching native job scripts, on the fly, to insert
completion messaging in the appropriate places, and other variables if
necessary, before invoking the (now modified) native process. A
significant advantage of this method is that you don't need to
permanently modify the model or its associated native scripting 
for cylc. Another is that you can configure the native job setup 
for a single test case (running it without cylc) and then have 
your custom wrapper modify the test case on the fly with suite, task,
and cycle-specific parameters as required.

To make this easier, for tasks that declare manual completion 
messaging, cylc makes non user-defined environment scripting
available in a single variable called
\lstinline=$CYLC_SUITE_ENVIRONMENT= that can be inserted into the
aforementioned native task scripts prior to calling the cylc messaging
commands.\footnote{Note that \lstinline=$CYLC_SUITE_ENVIRONMENT= is 
a string containing embedded newline characters and it has
to be handled accordingly. In the bash shell, for instance, it should
be echoed in quotes to avoid concatenation to a single line.}

\subsubsection{A Custom Task Wrapper Example}

The {\em detaching} example suite contains a script 
\lstinline=model.sh= that runs a pseudo-model executable as follows:
\lstset{language=bash}
\lstinputlisting{../examples/detaching/native/model.sh}
this is in turn executed by a script \lstinline=run-model.sh= that
detaches immediately after job submission (i.e.\ it exits before the
model executable actually runs):
\lstinputlisting{../examples/detaching/native/run-model.sh}
{\em Note that your {\bf at} scheduler daemon must be up 
if you want to test this suite.}

Here's a cylc suite to run this unruly model:
\lstset{language=suiterc}
\lstinputlisting{../examples/detaching/suite.rc}
\lstset{language=bash}
The suite invokes the task by means of the custom wrapper 
\lstinline=model-wrapper.sh= which modifies, on the fly, 
a temporary copy of the model's native job scripts as described above:
\lstinputlisting{../examples/detaching/bin/model-wrapper.sh}
\lstset{language=transcript}
If you run this suite, or submit the model task alone with
\lstinline=cylc submit=, you'll find that the usual job submission 
log files for task stdout and stderr end before the task is finished. 
To see the ``model'' output and the final task completion message
(success or failure), examine the log files generated by the 
job submitted internally to the {\em at} scheduler (their 
location is determined by the \lstinline=$PREFIX= variable in the
suite.rc file). 

It should not be difficult to adapt this example to real tasks
with detaching internal job submission.  You will probably also need to
replace other parameters, such as model input and output filenames, with
suite- and cycle-appropriate values, but exactly the same technique can
be used: identify which job script needs to be modified and use text
processing tools (such as the single line {\em perl} search-and-replace 
expressions above) to do the job.

       
%\pagebreak

\section{Task Job Submission}
\label{TaskJobSubmission}

{\em Task Implementation} (Section~\ref{TaskImplementation}) describes
what requirements a command, script, or program, must fulfill in order
to function as a cylc task. This section explains how tasks are submitted 
by cylc when they are ready to run, and how to define new task job
submission methods.

\subsection{Task Job Scripts}
\label{JobScripts}

When a task is ready to run cylc generates a temporary {\em task job
script} to configure the execution environment and call the task's
command scripting. The job script is the final result of suite.rc task 
configuration, in particular of the runtime namespace inheritance 
hierarchy for the task.  The script is submitted to run by means of the
{\em job submission method} specified for the task.  Different tasks can
have different job submission methods. Like other namespace properties,
you can set a default for the suite and override it on a per task basis:
\lstset{language=suiterc}
\begin{lstlisting}
# SUITE.RC
[runtime]
   [[root]] # suite defaults
        [[[job submission]]]
            method = loadleveler
   [[foo]] # just task foo
        [[[job submission]]]
            method = at_now 
\end{lstlisting}

The actual command line used to submit the job script is written to
stdout by cylc. In the following shell transcript we generate a job
script for a task in the QuickStart.c example suite and then examine it:
\lstset{language=transcript}
\begin{lstlisting}
% cylc submit --dry-run QuickStart.c Model%2011080506
> JOB SCRIPT: ~/cylc-run/QuickStart.c/log/job/Model%2011080506-1317298378.191123
> THIS IS A DRY RUN. HERE'S HOW I WOULD SUBMIT THE TASK:
~/cylc-run/QuickStart.c/log/job/Model%2011080506-1317298378.191123 </dev/null 
    1> ~/cylc-run/QuickStart.c/log/job/Model%2011080506-1317298378.191123.out 
    2> ~/cylc-run/QuickStart.c/log/job/Model%2011080506-1317298378.191123.err &
\end{lstlisting}
And here is the generated job script:
\lstset{language=bash}
\begin{lstlisting}
#!/bin/bash

# ++++ THIS IS A CYLC TASK JOB SCRIPT ++++
# Task: Model%2011080506
# To be submitted by method: 'background'

echo "TASK JOB SCRIPT STARTING"

# CYLC LOCATION, SUITE LOCATION, SUITE IDENTITY:
export CYLC_DIR=/home/oliverh/cylc
export CYLC_MODE=submit
export CYLC_SUITE_HOST=oliverh-33586DL.greta.niwa.co.nz
export CYLC_SUITE_PORT=NONE
export CYLC_SUITE_DEF_PATH=$HOME/cylc/examples/QuickStart/c
export CYLC_SUITE_REG_NAME=QuickStart.c
export CYLC_SUITE_REG_PATH=QuickStart/c
export CYLC_SUITE_OWNER=oliverh
export CYLC_USE_LOCKSERVER=False
export CYLC_UTC=False

# TASK IDENTITY:
export CYLC_TASK_ID=Model%2011080506
export CYLC_TASK_NAME=Model
export CYLC_TASK_CYCLE_TIME=2011080506
export CYLC_TASK_NAMESPACE_HIERARCHY="root Models Model"

# ACCESS TO CYLC:
PATH=$CYLC_DIR/bin:$PATH
# Access to the suite bin dir:
PATH=$CYLC_SUITE_DEF_PATH/bin:$PATH
export PATH

# SET ERROR TRAPPING:
set -u # Fail when using an undefined variable
# Define the trap handler
HANDLE_TRAP() {
  echo Received signal "$@"
  cylc task failed "Task job script received signal $@"
  trap "" EXIT
  exit 0
}
# Trap any signals which could cause the script to exit
trap "HANDLE_TRAP EXIT" EXIT
trap "HANDLE_TRAP ERR"  ERR
trap "HANDLE_TRAP TERM" TERM
trap "HANDLE_TRAP XCPU" XCPU

# SEND TASK STARTED MESSAGE:
cylc task started || exit 1

# ENVIRONMENT:
TASK_EXE_SECONDS="5"
WORKSPACE="/tmp/$USER/$CYLC_SUITE_REG_NAME/common"
MODEL_INPUT_DIR="$WORKSPACE"
MODEL_OUTPUT_DIR="$WORKSPACE"
MODEL_RUNNING_DIR="$WORKSPACE/Model"
export TASK_EXE_SECONDS WORKSPACE MODEL_INPUT_DIR MODEL_OUTPUT_DIR MODEL_RUNNING_DIR

# TASK COMMAND SCRIPTING:
Model.sh

# SEND TASK SUCCEEDED MESSAGE:
cylc task succeeded

echo "JOB SCRIPT EXITING (TASK SUCCEEDED)"
trap "" EXIT

#EOF
\end{lstlisting}

You can also generate a job script and print it directly to stdout, with
\lstinline=cylc jobscript=.


\subsection{Available Methods}
\label{AvailableMethods}

There are two basic job submission methods that should be available on
any platform, sufficient for running cylc's example suites if not real
forecasting systems:

\begin{myitemize}

    \item \lstinline=background= - run tasks directly in a background shell.

     \item \lstinline=at_now= - submit tasks to the rudimentary
         \lstinline=at= scheduler (\lstinline=atd= must be running).

\end{myitemize}

Tasks in a real forecasting system should be submitted to a batch queue
scheduler or cross-platform resource manager such as {\em loadleveler}
(IBM). Methods currently available are:

\begin{myitemize} 
    
    \item \lstinline=loadleveler= - This method submits general
        (non loadleveler-specific) task scripts to loadleveler. 
        Any {\em directives} you provide in the 
        suite.rc file 
        will be written to the job script, which will then 
        be submitted to run via \lstinline=llsubmit=. 

    \item \lstinline=ll_raw= - This method submits loadleveler-ready
        scripts (i.e.\ scripts containing hardwired directives) to
        loadleveler.  This may be necessary for complex (e.g.\
        multi-step) jobs. The original script is copied to
        make the temporary job script, and cylc environment
        scripting is inserted into it immediately after the loadleveler
        directives.

    \item \lstinline=ll_ecox= - This is derived from the basic 
        \lstinline=loadleveler= method. It automatically adapts certain
        task parameters (such as owner username) to NIWA's EcoConnect
        operational environment so that the same suite definition
        can be used in distinct {\em oper}, {\em test,} and {\em devel}
        environments in which the suite and task owners, and their home
        directories, vary accordingly.

\end{myitemize}


\subsection{Whither Task stdout And stderr?}
\label{WhitherStdoutAndStderr}

When a task is ready to run cylc generates task-specific stdout and
stderr filenames containing the task name, cycle time, and a string
of digits (seconds since epoch) to ensure uniqueness - rerunning the
task won't overwrite any old output:
\lstset{language=bash}
\begin{lstlisting}
# task job script:
~/cylc-run/QuickStart.c/log/job/Model%2011080506-1317298378.191123
# task stdout:
~/cylc-run/QuickStart.c/log/job/Model%2011080506-1317298378.191123.out 
# task stderr:
~/cylc-run/QuickStart.c/log/job/Model%2011080506-1317298378.191123.err
\end{lstlisting}
(the job submission log directory is configurable in the suite.rc file).

How the stdout and stderr streams are directed into these files depends
on the job submission method. The \lstinline=background= method just uses
appropriate output redirection on the command line, as shown above. The
\lstinline=loadleveler= method writes appropriate directives to the job
script that is submitted to loadleveler.

Cylc obviously has no control over the stdout and stderr output from
tasks that do their own internal output management (e.g.\ tasks 
that submit internal jobs and direct their output to other files). 
For less internally complex tasks, however, these will be complete task
job logs. {\em They can be viewed updating in real time in the suite
control GUIs}. 

\subsection{Defining New Job Submission Methods}

Defining a new job submission method requires some minimal amount of
Python programming.  You can derive (in the sense of object oriented
programming inheritance) new methods from one of the existing ones, or
directly from cylc's job submission base class,
\lstset{language=transcript}
\begin{lstlisting}
$CYLC_DIR/lib/cylc/job_submission/job_submit.py
\end{lstlisting}
using the existing methods as examples. Most often this should 
merely be a matter of defining the command line used to execute the
aforementioned job scripts and using the provided stdout and stderr file
paths appropriately. For example, here is the entire class code for 
the \lstinline=background= method:

\lstset{language=Python}

\begin{lstlisting}
#!/usr/bin/env python

from job_submit import job_submit

class background( job_submit ):
    """
Run the task job script directly in a background shell.
    """
    # stdin redirection (< /dev/null) allows background execution on
    # remote hosts - ssh needn't wait for the process to finish.
    COMMAND_TEMPLATE = "%s </dev/null 1>%s 2>%s &"
    def construct_jobfile_submission_command( self ):
        command_template = self.job_submit_command_template
        if not command_template:
            command_template = self.COMMAND_TEMPLATE
        self.command = command_template % ( self.jobfile_path,
                                            self.stdout_file,
                                            self.stderr_file )
\end{lstlisting}

Here is the \lstinline=at_now= method:

\begin{lstlisting}
#!/usr/bin/env python

from job_submit import job_submit

class at_now( job_submit ):
    """
Submit the task job script to the simple 'at' scheduler. The 'atd' daemon
service must be running.
    """
    COMMAND_TEMPLATE = "echo \"%s 1>%s 2>%s\" | at now"
    def construct_jobfile_submission_command( self ):
        command_template = self.job_submit_command_template
        if not command_template:
            command_template = self.COMMAND_TEMPLATE
        self.command = command_template % ( self.jobfile_path,
                                            self.stdout_file,
                                            self.stderr_file )
\end{lstlisting}

Finally, even the \lstinline=loadleveler= method is quite simple:

\begin{lstlisting}
#!/usr/bin/env python

from job_submit import job_submit

class loadleveler( job_submit ):
    """
Minimalist loadleveler job submission.
    """
    COMMAND_TEMPLATE = "llsubmit %s"

    def set_directives( self ):
        self.directive_prefix = "# @ "
        self.final_directive  = "# @ queue"

        defaults = {}
        defaults[ 'job_name' ] = self.task_id

        defaults[ 'output'   ] = self.stdout_file
        defaults[ 'error'    ] = self.stderr_file
        defaults[ 'shell'    ] = '/bin/ksh'

        # In case the user wants to override the above defaults:
        for d in self.directives:
            defaults[ d ] = self.directives[ d ]
        self.directives = defaults

    def construct_jobfile_submission_command( self ):
        command_template = self.job_submit_command_template
        if not command_template:
            command_template = self.COMMAND_TEMPLATE
        self.command = command_template % ( self.jobfile_path )
\end{lstlisting}

To use your new method, save it in a source file with the same name
as the job submission class (see examples above), install it in the cylc
source tree,
\lstset{language=transcript}
\begin{lstlisting}
$CYLC_DIR/lib/cylc/job_submission/MyNewJobSubmitMethod.py
\end{lstlisting}
and, in the spec file \lstinline=$CYLC_DIR/conf/suiterc.spec=, 
add its name to the list of allowed values for the 
suite.rc {\em job submission method} configuration items at suite level
and in the tasks section.

%\pagebreak

\section{Other Topics In Brief}

For more help on any topic, see cylc command line help (which is
comprehensive, and is included verbatim in
Section~\ref{CommandReference}); the suite.rc reference
(Section~\ref{SuiteRCReference}); the gcylc help menus; and the {\em
Quick Start Guide} (Section~\ref{QuickStartGuide}); and all of the 
examples suites installed into the central suite database.

\begin{myitemize}
    \item The difference between cold-, warm-, raw-, and re-starting, a suite
        (see \lstinline=cylc run help=)

    \item Intervening in suites, e.g.\ stopping, removing, inserting tasks; 
        (see \lstinline=cylc control help=)

    \item Interrogating suites and tasks 
        (\lstinline=cylc info help=, \lstinline=cylc show help=,
        and \lstinline=cylc discovery help=)

    \item Understanding cylc suite evolution, particularly in catch up
        operation (the {\em Quick Start Guide} will also help here,
        along with the cylc example suites, and running your real suites
        in simulation mode)

    \item suite security - use of secure passphrases (this is
        trivial to configure, see documentation of the {\em use secure
        passphrase} configuration item in the {\em Suite.rc Reference},
        Appendix~\ref{SuiteRCReference})

    \item automatic state dump backups, named pre-intervention state dumps
        (mentioned in the {\em Quick Start Guide}; watch the suite log
        after intervening in a suite)

    \item centralized alerting and timeouts
        (see documentation of {\em task event hooks} in the {\em
        Suite.rc Reference}, Appendix~\ref{SuiteRCReference})

    \item Recursive purge - this is a powerful suite
        intervention but you need to understand how it works before
        using it. See \lstinline=cylc purge help= for details.

    \item Handling spin-up processes via temporary tasks
        and adding prerequisites on-the-fly - see 
        \lstinline=cylc depend --help=, and note that when you insert a
        task into a running suite (a) the initial cycle time can be in
        the past; and (b) you can give a final cycle time after which the
        task will be eliminated from the suite. 
    % \item How to recover from certain kinds of task failure.
    %\item instant simulation mode clones of a running suite
    %\item fuzzy prerequisites - triggering of the {\em most recent available},
    %    within limits, instance of an upstream task
    \item Sub-suites - running another suite inside a task:
\lstset{language=suiterc}
\begin{lstlisting}
[runtime]
    [[foo]]
        command scripting = "cylc run SUITE $CYLC_TASK_CYCLE_TIME --until=$CYLC_TASK_CYCLE_TIME"
\end{lstlisting}
\end{myitemize}
\lstset{language=transcript}
%\pagebreak

\section{Suite Design Principles}
\label{SuiteDesignPrinciples}

%Simplicity, flexibility, efficiency, and portability of cylc suites.

\subsection{Make Fine-Grained Suites} 
\label{Granularity}

A suite can contain a small number of large, internally complex tasks; a
large number of small, simple tasks; or anything in between. Cylc can
easily handle a large number of tasks, however, so there are definite
advantages to fine-graining:

\begin{myitemize}
    \item a more modular and transparent suite.

    \item better functional parallelism (multiple tasks running
        at the same time).

    \item faster debugging and failure recovery: rerun just the tasks(s)
        that failed. 

    \item code reuse: similar tasks can often call the same script or
        command with differing task-specific input parameters
        (consider tasks that move files around, for example).

\end{myitemize}

\subsection{Use Include-Files For Groups Of Related Tasks}

Suite.rc include-files can be used just to help organise tasks into
convenient groups in very large suites (and with \lstinline=cylc edit= 
you can edit a temporarily inlined file to get a global view). 
But they are most useful for handling the repetitive definition of
groups of similar tasks, because the same inclusion can be used multiple
times in the same suite.rc file.  See {\em Include Files}
(Section~\ref{IncludeFiles}).


\subsection{Make Tasks Rerunnable}

It should be possible to rerun a task by simply resubmitting it for the
same cycle time. In other words, failure at any point during execution
of a task should not render a rerun impossible by corrupting the state
of some internal-use file, or whatever. It's difficult to overstate the
usefulness of being able to rerun the same task multiple times,
either outside of the suite with \lstinline=cylc submit=, or by
retriggering it within the running suite, when debugging a problem.

\subsection{Make Models Rerunnable} 

If a warm-cycled model simply overwrites its restart files in each
run, the only cycle that can subsequently run is the next one. This
is dangerous because if, accidentally or otherwise, the task runs for the
wrong cycle time, its restart files will be corrupted such that the
correct cycle can no longer run (probably necessitating a cold-start).
Instead, consider organising restart files by cycle time, through a file
or directory naming convention, and keep them in a simple rolling
archive (cylc's filename templating and housekeeping
utilities can easily do this for you). Then, given availability of 
any external inputs, you can easily rerun the task for any cycle still
in the restart archive.

\subsection{Limit Previous-Instance Dependence} 
\label{LimitPID}

Cylc does not require that successive instances of the same task run 
sequentially. In order to task advantage of this and achieve maximum
functional parallelism whenever the opportunity arises (usually when 
catching up from a delay) you should ensure that tasks that in
principle do not depend on their own previous instances (the vast
majority of tasks in most suites, in fact) do not do so in practice. In
other words, they should be able to run as soon as their prerequisites
are satisfied regardless of whether or not their predecessors have
finished yet.  This generally just means ensuring that all file I/O
contains the generating task's cycle time in the file or directory name
so that there is no interference between successive instances. If this
is difficult to achieve in particular cases, however, you can declare
the offending tasks to be {\em sequential}.

% MAYBE SHOULD INCLUDE THE FOLLOWING HERE:
%Warm-cycled forecast models {\em do} depend on their own previous
%instances (through their ``model background'' restart prerequisites).
%These can be made to run sequentially (i.e.\ with maximal previous
%instance dependence) but you can have cylc suite launch the next model,
%assuming other prerequisites are satisfied, as soon as the previous one
%has completed its restart prerequisites (minimal previous instance
%dependence, maximal throughput).

\subsection{Put Task Cycle Time In All Output File Paths}
\label{PutCycleTimeinIO}

Having all filenames, or perhaps the names of their containing
directories, stamped with the cycle time of the generating task greatly
aids in managing suite disk usage, both for archiving and cleanup. It
also enables the aforementioned task rerunnability recommendation by avoiding
overwrite of important files from one cycle to the next. Cylc has 
powerful utilities for cycle time offset based filename templating and
housekeeping.

\subsubsection{Use Cylc's Cycle Time Filename Template Utility}

The command line utility program \lstinline=cylc template=
determines filenames based on a template string containing
YYYYMMDDHH (or variations thereof) by substituting the
current cycle time, or some offset from it, into the template.
This can be used in the suite.rc environment sections, or in
task implementation scripts if necessary, to instantly generate cycle
time appropriate filenames for any purpose in the suite.

See \lstinline=cylc util template help= for more information.

\subsection{How To Manage Input/Output File Dependencies}
\label{HandlingDependencies}

Dependencies between tasks usually, though not always, take the form of
files generated by one task that are used by other tasks. It is possible
to manage these files across a suite without hard wiring I/O locations 
and therefore comprising suite flexibility and portability.

\begin{myitemize}

\item {\bf Use A Common I/O Workspace}

For small suites you may be able to have all tasks read and write from a
common workspace, thereby avoiding the need to move common files around.
You should be able to define the workspace location once in the suite.rc
file rather than hard wiring it into the task implementations.

\item {\bf Add Connector Tasks To The Suite} 

Tasks can be added to a suite to move files from A's output directory to
B's input directory, and so on.  These connector tasks may all be able
to call the same file transfer script or command, with differing input
parameters defined in the suite.rc file. 

\item {\bf Dynamic Configuration Of I/O Paths}

Whether or not your suite uses a single common workspace, passing common
I/O paths to tasks via variables defined once in the suite.rc file should
allow you to avoid using connector tasks at all, except where it is
necessary to transfer files between machines, or similar. 

\end{myitemize}

\subsection{Use Generic Task Scripts}

If your suite contains multiple logically distinct tasks that actually
have similar functionality (e.g.\ for moving files around, or for 
generating similar products from the output of several similar models)
have the corresponding cylc tasks all call the same command, script, or
executable - just provide different input parameters
via the task command scripting and/or execution environment, in the
suite.rc file.


\subsection{Make Suites Portable}

If every task in a suite is configured to put its output under
\lstinline=$HOME= (i.e.\ the environment variable, literally, not the
explicit path to your home directory; and similarly for temporary
directories, etc.) then other users will be able to copy the suite and
run it immediately, after merely ensuring that any external input files
are in the right place.

For the ultimate in portability, construct suites in which all task I/O
paths are dynamically configured to be user and suite (registration)
specific, e.g.
\begin{lstlisting}
$HOME/output/$CYLC_SUITE_REG_PATH
\end{lstlisting}
(these variables are automatically exported to the task execution
environment by cylc - see {\em Task Execution Environment},
Section~\ref{TaskExecutionEnvironment}). Then you can run multiple
instances of the suite
at once (even under the same user account) without changing anything,
and they will not interfere with each other.

{\em You can test changes to a portable suite safely by making a quick copy
of it in a temporary directory, then modifying and running the test copy 
without fear of corrupting the output directories, suite logs, and 
suite state, of the original.} 


\subsection{Make Tasks As Self-Contained As Possible}

Where possible, no task should rely on the action of another task,
except for the prerequisites embodied in the suite dependency graph that
it has no choice but to depend on. If this rule is followed, your suite
will be as flexible as possible in terms of being able to run single
tasks, or subsets of the suite, whilst debugging or developing new
features.\footnote{The \lstinline=cylc submit= command runs a single
task exactly as its suite would, in terms of both job submission method and
execution environment.}  For example, every task should create its own
output directories if they do not already exist, instead of assuming
their existence due to the action of some another task; then you will be
able to run single tasks without having to manually create output
directories first. 

\begin{lstlisting}
# manual task scripting:
  # 1/ create $OUTDIR if it doesn't already exist:
  mkdir -p $OUTDIR
  # 2/ create the parent directory of $OUTFILE if it doesn't exist:
  mkdir -p $( dirname $OUTFILE )

# OR using the cylc checkvars utility:
  # 1/ check vars are defined, and create directories if necessary:
  cylc util checkvars -c OUTDIR1 OUTDIR2 #...
  # 2/ check vars are defined, and create parent dirs if necessary:
  cylc util checkvars -p OUTFILE1 OUTFILE2 #...
\end{lstlisting}

\subsection{Make Suites As Self-Contained As Possible}

The only compulsory content of a cylc suite definition directory is the
suite.rc file (and you'll almost certainly have a suite
\lstinline=bin= sub-directory too). However, you can store whatever you
like in a suite definition directory;\footnote{If you copy a suite using
cylc commands or gcylc, the entire suite definition directory
will be copied.} other files there will be ignored by cylc but
suite tasks can access them via the
\lstinline=$CYLC_SUITE_DEF_PATH= variable that cylc automatically 
exports into the task execution environment. Disk space is cheap - if
all programs, ancillary files, control files (etc.) required by the
suite are stored in the suite
definition directory instead of having the suite reference external
build directories (etc.), you can turn the directory into a revision
control repository and be virtually assured of the ability to exactly
reproduce earlier versions as required, regardless of suite
complexity.

\subsection{Orderly Product Generation?}
\label{OrderlyProductGeneration}

Correct scheduling is not equivalent to ``orderly generation of products
by cycle time''.  Under cylc, a product generation task will trigger as
soon as its prerequisites are satisfied (i.e.\ when its input files are
ready, generally) regardless of whether other tasks with the same cycle
time have finished or have yet to run. If your product delivery or
presentation system demands that all products for one cycle time are
uploaded (or whatever) before any from the next cycle, then be aware
that this may be quite inefficient if your suite is ever faced with
catching up from a significant delay or running over historical data.

If you must, however, you can introduce artificial dependencies into
your suite to ensure that the final products never arrive out of
sequence.  One way of doing this would be to have a final ``product
upload'' task that depends on completion of all the real product
generation tasks at the same cycle time, and then declare it to be
sequential so that successive instances cannot run out
of sequence, or in parallel, even if the opportunity arises.

\subsection{Clock-triggered Tasks Wait On External Data}

All tasks in a cylc suite know their own private cycle time, but most
don't care about the wall clock time - they just run when their
prerequisites are satisfied. The exception to this is {\em
clock-triggered} tasks, which wait on a wall clock time expressed as an
offset from their own cycle time, in addition to any other
prerequisites. The usual purpose of these tasks is to retrieve real time
data from the external world, triggering at roughly the expected time of
availability of the data. Triggering the task at the right time is up to
cylc, but the task itself should go into a check-and-wait loop in case
the data is delayed; only on successful detection or retrieval should
the task report success and then exit (or perhaps report failure and
then exit if the data has not arrived by some cutoff time). 

\subsection{Do Not Treat Real Time Operation As Special} 

Cylc suites, without modification, can handle real time and delayed
operation equally well.

In real time operation clock-triggered tasks constrain the
behaviour of the whole suite, or at least of all tasks 
downstream of them in the dependency graph.

In delayed operation (due to an actual delay in an operational suite or
because you're running an historical case study) clock-triggered tasks
will not constrain the suite at all, and cylc's multi-cycling abilities
come to the fore, because their trigger times have already passed. 
But if a clock-triggered task happens to catch up to the wall clock, it
will automatically wait again. In this way a cylc suite naturally
and seamlessly transitions between delayed and real time operation 
as required.

\pagebreak

\appendix

\input{suiterc.tex}

\pagebreak

\section{Command Reference}
\label{CommandReference}

%This section is auto-generated from the self-documenting command set.
  
\lstset{language=usage}
\input{commands.tex}
\lstset{language=transcript}

\section{The Cylc Lockserver}
\label{TheCylcLockserver}

Each cylc user can optionally run his/her own lockserver to prevent
accidental invocation of multiple instances of the same suite or task at
the same time. The suite and task locks brokered by the lockserver are
analogous to traditional lock files, but they work across a network,
even for distributed suites containing tasks that start executing on one
host and finish on another.  

Accidental invocation of multiple instances 
of the same suite or task at the same time can have serious consequences, 
so use of the lockserver should be considered for important operational
suites, but it may be considered an unnecessary complication for general 
less critical usage, so it is currently disabled by default. 

To enable the lockserver:
\lstset{language=suiterc}
\begin{lstlisting}
# SUITE.RC
use lockserver = True
\end{lstlisting}

The suite will now abort at start-up if it cannot connect to the
lockserver.  To start your lockserver daemon, 
\lstset{language=transcript}
\begin{lstlisting}
% cylc lockserver start
\end{lstlisting}

To check that it is running,
\begin{lstlisting}
% cylc lockserver status
\end{lstlisting}

For detailed usage information,
\begin{lstlisting}
% cylc lockserver --help 
\end{lstlisting}

There is a command line client interface, 
\begin{lstlisting}
% cylc lockclient --help 
\end{lstlisting}
for interrogating the lockserver and managing locks manually (e.g.\
releasing locks if a suite was killed before it could clean up after
itself).

To watch suite locks being acquired and released as a suite runs, 
\begin{lstlisting}
% watch cylc lockclient --print
\end{lstlisting}

\section{The Graph-Based Suite Control GUI}
\label{TheGraphBasedcontrolGUI}

The graph-based suite control GUI has the advantage that it shows
the structure of a suite very clearly as it evolves. It works 
remarkably well even for very large suites, on the order of one hundred
tasks or more {\em but} on the downside, the graphviz engine does a new
global optimization every time the graph changes, so the layout is often
not very stable. This may or may not be a solvable problem - it seems
likely that making continual incremental changes to an existing graph
without redoing the global layout would inevitably result in some kind
of horrible mess.

The following features of the graph-based control GUI go a long way 
toward mitigating the changing layout problem:

\begin{myitemize}
    \item The disconnect button can be used to temporarily prevent the
        graph from changing as the content of the suite changes (and
        in real time operation suites evolve quite slowly anyway)
    \item Right-click on a task and choose the ``Focus'' option to restrict
        the graph display to that task's cycle time. Anything interesting
        happening in other cycles will show up as disconnected
        rectangular nodes to the right of the graph (and you can click
        on those to intantly refocus to their cycles).
    \item Task filtering is the ultimate quick route to temporarily focusing
        on just the tasks you're interested in (but this will 
        destroy the graph structure, to state the obvious).
\end{myitemize}

In future cylc releases we plan to keep the graph centered, after layout
changes, on the most recently clicked-on task.


\section{Simulation Mode} 
\label{SimulationMode}

If you start a suite in simulation mode (a command line option for the cold-,
warm-, raw-, and re-start commands, and a checkbutton in the gcylc suite
start panel) then cylc will run on an accelerated clock and submit dummy
programs instead of the real tasks. These masquerade as the real tasks
by reporting the correct outputs complete after a short interval,
reporting success, and then exiting. This is essentially
indistinguishable, to cylc, from real operation. Simulation mode was, and
remains, an important aid to cylc development because it allows
testing of every aspect of scheduling without having to run real
tasks in real time. Prior to cylc-3 it was also a useful aid to suite
development - a simulation run would quickly identify any mismatch between
the user-defined prerequisites and outputs across the suite, so you
could get the scheduling right without running the real tasks. Post
cylc-3.0 this is less important because task prerequisites and outputs
are implicitly defined by the dependency graph and, short of a bug in
cylc, the suite will run according to the graph.

\subsection{Clock Rate and Offset}

Simulation mode suites run on an accelerated clock so that you can test
things very quickly. You can set the clock rate and offset with respect
to the initial cycle time with options to the \lstinline=cylc run=
command. An offset of 10 hours, say, means that the simulation mode clock
starts at 10 hours prior to the suite's initial cycle time.  You can
thus simulate the behaviour of the suite as it catches up from a delay
and transitions to real time operation.  By default, the clock runs at a
rate of 10 seconds real time to 1 hour suite time, and with an initial
offset of 10 hours. 

\subsection{Switching A Suite Between Simulation And Live Modes?}

The scheduler mode (simulation or live) is recorded in the suite state
dump file. {\em Cylc will not let you restart a simulation mode suite in
live, or a live mode suite in simulation mode} - any attempt to do the 
former must certainly be a mistake, and doing the latter, while
feasible, would corrupt your live suite state dump and turn it over to
simulation mode. Note, however, that if you really want to run the
current state of a live suite forward in simulation mode,
all you need to do is this:
\begin{myenumerate}
    \item back up the live state dump (or take note of the filename
        of the relevant automatic dump when you do the final suite
        shutdown intervention).
    \item edit the mode line in the state dump file, and restart the
        suite in simulation mode.
    \item later, restart the live suite from the pre simulation mode
        state dump backup 
\end{myenumerate}

{\em Cylc can also create an instant dummy mode clone of the current
state of any running or stopped suite, but this ``practice mode'' has
been disabled in the current release pending testing.}

%\subsubsection{Practice Mode}
%
%Practice mode allows quick and easy testing of potentially complex
%suite interventions, with complete safety.
%
%\begin{lstlisting}
%cylc restart --practice examples:CUG_1
%\end{lstlisting}
%
%This will start a simulation mode clone of an existing suite from the
%current state of that suite (which may be paused, still running, or
%halted), but using different state and log files so that the original
%suite will not be corrupted by the clone.
%
%{\em At start-up in practice mode, failed tasks are not reset to waiting}
%because the whole point of practice mode is to ``practice'' how to
%recover from failures.
%
%Note that other cylc commands for monitoring or interacting with the
%suite must also use the \lstinline=--practice= option in order to
%target the practice suite and not the real one. Be sure to set
%\lstinline=cylc lock= on the original suite first, to avoid
%accidentally messing with it (even if you do screw up, however, cylc's
%automatic pre-intervention state dumps will save you!).
%
%
%\subsubsection{Roll Your Own Practice Mode}
%
%A less automated way to ``practice'' on a copy of an existing suite
%that starts up from the current (or previous) state of that suite, 
%\lstinline=cylc run --practice= is this:
%
%\begin{myitemize}
%    \item register your suite again under a different name. This allows
%        you to run a simulation mode copy of the same suite without
%        interfering with the original suite (it also allows you to run
%        a copy of the live mode suite without interference, but only if
%        the real suite tasks are configured to use the registered
%        suite name in all important input and output filenames and/or
%        directory paths - see {\em Command Reference} Section~\ref{register}).
%
%    \item start-up the newly registered suite in simulation mode using:
%        \begin{lstlisting}
%cylc restart --simulation-mode SUITE PATH
%        \end{lstlisting}
%        where PATH is a state dump file from the original suite. The
%        absolute path is required here because the default state
%        dump location depends on the registered suite name (so that
%        different suites don't interfere with each other's state
%        dumps).
%
%\end{myitemize}
%

        

%\pagebreak

%\section{Network Issues}
%
%Cylc can control tasks on a distributed system (multiple hosts).  If you
%do have a distributed suite, in any or all of these ways, be aware of
%the following issues: 
%
%\begin{myitemize}
%
%    \item In addition to the cylc host, cylc must be installed on all
%        task hosts; and the remote task scripts must
%        themselves be installed on their host machines.  Refer to {\em
%        Running Tasks On A Remote Host}
%        (Section~\ref{RunningTasksOnARemoteHost}) for more on this.
%
%    \item Pyro must be installed on every host used by the suite.
%         Ideally all relevant machines should have the same version of
%         Pyro, but you can easily check for Pyro cross-version
%         compatibility by attempting to run one of the cylc example
%         system.
%        
%\end{myitemize}
%
%Other notes relevant to the last point above: the \lstinline=--host=
%cylc command option defaults to Python \lstinline=socket.getfqdn()=,
%which retrieves the fully qualified domain name of the local host
%if possible.  But \lstinline=/etc/hosts= may cause this to return
%just the hostname, which locally may resolve to the local-only IP
%address that is not accessible on the network.  Short of reconfiguring
%the hosts file, you may be able to workaround these problems by:
%
%\begin{myitemize}
%
%    \item and, get cylc to configure the Pyro daemon 
%        with \lstinline@Pyro.config.PYRO_DNS_URI = True@ 
%        
%    \item and, use the \lstinline=--host= cylc command option where
%        required.
%
%\end{myitemize}

\section{Cylc Development History}
\subsection{Pre-3.0}

Early versions of cylc were focused on developing and testing the new 
scheduling algorithm, and the suite design interface at the time was
essentially the quickest route to that end. A suite was a collection
of ``task definition files'' that encoded the prerequisites and outputs
of each task in a direct reflection of cylc's internal task proxies. 
This way of defining suites exposed cylc's self-organising nature to the
user, and it did have some nice properties. For instance a group of
tasks could be transferred directly from one suite to another by simply
copying the taskdef files over (and checking that prerequisite and
output messages were consistent with the new suite). However, ensuring
consistency of prerequisites and outputs across a large suite could be
tedious; a few edge cases associated with suite start-up and forecast
model restart dependencies were, arguably, difficult to understand; and
the global structure of a suite was not readily apparent until run time
(although to counter this cylc 2.x could generate run-time resolved
dependency graphs very quickly in simulation mode).

\subsection{Version 3.0}

Version 3.0 implemented an entirely new suite design interface in which
one defines the suite dependency graph, execution environment, and
command scripting for each task, in a single structured, validated,
configuration file - the suite.rc file.  This {\em really} makes suite
structure apparent at a glance, and task prerequisites and outputs (and
some other important parameters besides) no longer need to be specified
by the user because they are implied by the graph.

\subsection{Version 4.0}

Version 4.0 has the following major improvements over cylc-3.x, along with
many refinements: 
\begin{myitemize}
    \item suite registration has been generalized from GROUP:NAME to a 
        hierarchy of arbitrary depth, e.g foo.bar.baz, allowing suites
        to be organized in a tree-like structure.
    \item the suite.rc file has undergone some major housekeeping and, 
        in particular, now defines a {\em namespace hierarchy} of task
        families and tasks, to allow common run time properties to be
        grouped naturally among related tasks.
\end{myitemize}

\section{Pyro} 
\label{Pyro}

Pyro (Python Remote Objects) is a widely used open source objected
oriented Remote Procedure Call technology developed by Irmen de Jong.

Earlier versions of cylc used the Pyro Nameserver to handle marshalling
of communication between client programs (tasks, commands, viewers,
etc.) and their target suites. This worked well, but in principle it
provided a route for one suite or user on the subnet to bring down 
all running suites by killing the nameserver. Consequently cylc now
uses Pyro simply as a lightweight object oriented wrapper for
direct network socket communication between client programs and their
target suites - all suites are thus entirely isolated from one another. 

%\section{Known Limitations}
%
%\begin{myitemize}
%    \item {\em The minimum cycle time granularity for a task is
%        currently one hour}.
%
%Note that this is only loosely connected to when a task actually runs,
%and then only in real time operation: clock-triggered tasks trigger at
%some interval beyond their nominal cycle time, which in real time
%operation will be at most hourly for a given task. Aside from this, tasks
%can run as soon as their prerequisites are satisfied regardless of their
%nominal cycle time. This minimum granularity could be extended to
%minutes and seconds if necessary.  
%
%    \item {\em Every task must run at least once per day} (of cycle
%        time, not real time, although the first implies the second in
%        real time operation). 
%
%This could be extended to allow less frequent tasks, e.g.\ once per
%week, if required. 
%
%\end{myitemize}

%\section{Miscellaneous Notes}
%\label{MiscellaneousNotes}
%
%%\subsection{Cycle Dependent Prerequisites}
%%It may be better to split a task into two (or more) than use this 
%%capability. NOT FOR FORECAST MODELS WITH RESTART OUTPUTS!

%\subsection{Catching Up}
%labelsubsection{CatchingUp}
%
%The state of being ``caught up'' or not is a property of individual
%tasks, not the whole suite, and additionally it should only matter to
%external contact tasks, i.e.\ those that wait on external data that is
%available at a wall clock time of T (task cycle time) + o (some offset
%insterval). Where this matters an external task can detect whether or
%not it has caught up (and signal this to its proxy object in cylc) by
%comparing its cycle time (and offset) to the wall clock time.  


%\section{Known Bugs At Version THIS IS NOT A VERSIONED RELEASE}
%
% FIXED?:
%\begin{myitemize}
%    \item Sometimes the gcylc database view does not update when a suite
%        or group has been unregistered by another process. Workaround:
%        restart gcylc.
%\end{myitemize}

\section{Acknowledgements}

Bernard Miville and Phil Andrews (NIWA) for discussion, bug finding, and
many suggestions that have improved cylc's usability and functionality;
David Matthews and Matthew Shin (Met Office UK) for the same, and much
code development as well.

\section{GNU GENERAL PUBLIC LICENSE v3.0}
\input{gpl-3.0}



%\subsection{Understanding Suite Evolution}
%\label{UnderstandingSuiteEvolution}
%
%On a cold-start all tasks (including one-off tasks) in the sytem will be
%instantiated at the initial cycle time, or at the next subsequent valid
%cycle time for the task. Any tasks that have no prerequisites (and, if
%they are contact tasks, have reached their trigger time) will submit to
%run immediately. Any cycling (i.e.\ non one-off) tasks that have no
%prerequisites (and, if they are contact tasks, have reached their
%trigger time) will rapidly spawn ahead until stopped by the suite's
%runahead limit (observe task X in the User Guide example suite).
%Thereafter, each task will, of its own accord, submit to run as soon as
%its prerequisites have been satisfied by other tasks already running or
%succeeded in the suite (and trigger time etc.).  Each task spawns a
%successor at a point in its lifecycle that depends on its type: tied
%tasks spawn has soon as their restart prerequisites have been completed,
%and free tasks spawn at the instant they start running.  Once a task
%exists it is free to run as soon as its prerequisites are satisfied,
%thus successive instances of a free task can run entirely in parallel,
%and successive instances of a tied task can overlap if the opportunity
%arises (other prerequisites allowing).

%\subsection{Automatic State Dumps}
%\label{AutomaticStateDumps}
%
%Cylc updates its configured state dump file (e.g.\
%\lstinline=$HOME/cylc-state/state=) every time the state of a task
%changes. Previous states are maintained in a rolling archive 
%(length specified in the {\em suite.rc} file):
%
%\begin{lstlisting}
%nwp_oper> ls .cylc/state/SUITE/
%state       # current state
%state-1     # most recent previous state
%state-2     # next most recent previous state
%...
%state-N     # oldest state dump; will be deleted at next update
%\end{lstlisting}
%
%In addition, immediately prior to any system intervention a special
%uniquely named state dump file is created and logged, e.g.:
%
%\begin{lstlisting}
%2010/03/30 14:54:29 WARNING main - pre-purge state dump: state.2010:3:30:14:54:29
%\end{lstlisting}
%
%If you accidentally intervene wrongly in a suite, just shut it down
%and restart from the pre-intervention state dump:
%
%\begin{lstlisting}
%cylc restart SUITE state.2010:3:30:14:54:29
%\end{lstlisting}

%\subsection{Suite Log Files}
%\label{SuiteLogFiles}
%
%Earlier versions of cylc created a main suite log file and a
%task-specific log for every task. However, because when all logged
%events were made to percolate up to the main log the task-specific logs
%became superfluous. Instead, cylc provides facilities for filtering the
%main log for task-specific messages (or you can just use
%\lstinline=grep= for this purpose).
%
%\begin{lstlisting}
%$ tail $HOME/cylc-logs/intro/log
%2010/03/28 00:33:50 INFO main.F - [2010010312] disconnected (spent; general)
%2010/03/28 00:33:52 INFO main.C - [2010010400] storm surge fields ready for 2010010400
%2010/03/28 00:33:52 INFO main.A - [2010010412] surface wind fields ready for 2010010412
%2010/03/28 00:33:52 INFO main.C - [2010010400] C%2010010400 completed
%2010/03/28 00:33:52 INFO main.C - [2010010400] C%2010010400 succeeded
%2010/03/28 00:33:52 INFO main.A - [2010010412] surface pressure field ready for 2010010412
%2010/03/28 00:33:52 INFO main.A - [2010010412] level forecast fields ready for 2010010412
%2010/03/28 00:33:53 INFO main.A - [2010010412] A%2010010412 completed
%2010/03/28 00:33:53 INFO main.A - [2010010412] A%2010010412 succeeded
%2010/03/28 00:33:53 CRITICAL main - ALL RUNNING TASKS SUCCEEDED
%\end{lstlisting}
%
%Each entry shows the time of logging, the name and cycle time of the
%reporting task (in square brackets), and the logged message.
%
%In simulation mode, the logged time is the simulation mode accelerated clock time, not 
%real time.
%
%Existing log files are automatically rotated at start-up and,
%individually, when they reach a size of 10 MB.  This maximum file 
%size should be configurable, but it is currently hardwired in
%\lstinline=$CYLC_DIRsrc/pimp_my_logger.py=.

%\subsection{Diagnosing A Stalled Suite}
%\label{DiagnosingAStalledSuite}
%
%In certain situations a suite may appear to be ``stuck'', i.e.\ no
%tasks are running and nothing appears to be happening. There are several 
%possible reasons for this (it does not necessarily indicate a problem!):
%
%\begin{myitemize}
%    \item In {\em normal real time operation}, when all running tasks
%        have finished for the most recent cycle, nothing will happen
%        until the one or more contact tasks in the suite trigger at the
%        start of the next cycle. \lstinline=cylc show= tells if a
%        contact task has yet to reach its trigger time.
%
%    \item if every task in the suite has one or more unsatisfied
%        prerequisites, the suite will be stalled. This could happen,
%        for example, if you start a suite that contains tied (forecast
%        model) tasks without the corresponding one-off cold-start tasks to
%        satisfy their initial restart prerequisites.
%
%\end{myitemize}
%
%The following problems will eventually cause a suite to get stuck at
%the {\em runahead limit} (which by default is 24 hours: i.e.\ the
%fastest task in the suite is only allowed to get 24 hours ahead of the
%slowest) because cylc does not automatically remove failed tasks from
%the system.  Operational suites should have automated means of
%alerting the operators to any failure that occurs, but in the
%unlikely event that the failure is not noticed until the system stalls
%at the runahead limit, then to get things moving again the operator must
%either remove the failed task or reset (and thereby rerun) it after
%fixing the problem that cause the failure.
%
%\begin{myitemize}
%    \item If the system operator, perhaps in a post-task-failure
%    intervention, kills some tasks that are required to satisfy the
%    prerequisites of other tasks that still exist in the system, then 
%    the suite will eventually stall as a result of these tasks being
%    unable to run. Solution: insert tasks (possibly one-off cold-start
%    tasks) to get the suite running again.
%
%    \item If some task in the suite has a cycling interval long than
%        the runahead limit, the suite will stall (e.g.\ if
%    you have a task that runs 24-hourly at 00Z, but set the runahead
%    limit to just 12 hours). This could also happen if you purge enough
%    cycles that the difference between the pre- and post-purge tasks
%    is greater than the runahead limit. Solution: ensure your runahead
%    limit is large enough to span these gaps.
%
%    \item If a failed task has not yet been removed or reset by the
%    system operator it will eventually stall the suite. Solution:
%    Fix, or otherwise deal with, failed tasks as quickly as possible.
%
%    \item If through a suite design error error, a task exists that
%        cannot get its prerequisites satisfied by any other task in the
%        suite, that task will never run and will eventually cause the
%        suite to stall.  Solution: test the suite in simulation mode to 
%        check that all prerequisites and outputs, suite-wide, are 
%        compatible.
%
%    \item If a misconfigured external task does not report an output
%        that it is supposed to (i.e.\ as registered in its task proxy
%        definition file), then its task proxy will not record that 
%        output as complete and cylc will set it to the 'failed' state
%        when it finishes without completing a registered output. A
%        failed task will eventually stall the suite, as explained above, 
%        if it is not fixed and re-run, or removed from the suite.
%        Solution: ensure all external tasks report their outputs
%        correctly.
%
%\end{myitemize}
%
%To confirm that the runahead limit is causing a stall, you can use 
%\lstinline=cylc verbosity= to set the debug logging level: any task
%that is not spawning a successor only because it has exceeded the
%runahead limit will report that to the log.
%
%
%\subsection{Failure Recovery Scenarios}
%\label{FailureRecoveryScenarios}
%
%\begin{myitemize}
%    \item {\em One forecast cycle runs into the next, after a delay in
%        operations}. This is never a problem for cylc; every task runs
%        as soon as it can run, regardless of forecast cycle, and any
%        task that can't run before it's predecessor has finished will
%        wait.
%
%    \item {\em A delayed parallel trial or case study catches up to real
%        time operation}. This is no problem for cylc; any cylc suite
%        will seamlessly transition in and out of ``normal real time
%        operation'' (distinct cycles triggered by the wall clock) as needed.
%
%    \item {\em An external task fails, but can be fixed}. For example, a
%        forecast model aborts trying to read a corrupted data file that
%        can be regenerated correctly. The failed task will be noted by
%        cylc, and its downstream dependants will not be able to run,
%        but other tasks will carry on as normal while you address the
%        problem. When fixed, use `cylc reset' to get the failed task to
%        run again, after which it and its downstream dependants will
%        catch up to the rest of the suite as quickly as possible.
%
%    \item {\em An important external task fails, but cannot be fixed.}
%        In this case, if the task has a lot of downstream dependants,
%        you will presumably need omit one or more cycles of the affected
%        tasks, and cold-start their part of the suite at a the earliest
%        possible subsequent cycle.  To do this, insert the relevant cold
%        start task, or task group, at the later cycle, then purge the
%        failed task and everything that depends on it (and on them, and
%        so on) down to the cold-start time.  Other downstream forecast
%        models will be able to pick up immediately so long their most
%        recent previous instance (i.e.\ just before the gap) wrote out
%        sufficient restart outputs to bridge the gap (otherwise they,
%        or perhaps the entire suite, will need to be cold-started). 
%
%    \item {\em HELP, I attempted a drastic intervention in a complex
%        suite, using the horrifying purge command, and this time I
%        really screwed the pooch!} Before any operation that alters the
%        sytem state, cylc automatically writes out a special state dump
%        file and reports the filename in the main log. Shut the suite
%        down and restart it from its pre-intervention state (just
%        cut-and-paste the state dump filename from the main log file -
%        the file path is not required because the file will be in the
%        configured suite state dump directory).  Then {\em retry your
%        intervention in practice mode} before doing it for real!
%
%\end{myitemize}
%
%\subsection{Dead Suite Cleanup}
%%\label{Deadcleanup}
%
%\subsubsection{Normal Shutdown}
%
%Cylc waits for any currently running tasks to finish before shutting
%down cleanly. There will be nothing to clean up. 
%
%\subsubsection{Shutdown NOW or Controlled Abort}
%
%If a critical error of some kind, or use of \lstinline=cylc stop --now=,
%results in an immediate suite shutdown while there are still external
%tasks running, any subsequent cylc messaging calls made by the
%still-running tasks will fail because the parent suite no longer
%exists. Depending on the exact circumstances this may result in some 
%orphaned processes that need to be killed manually.
%
%\subsubsection{Uncontrolled Suite Abort}
%
%(To Do, check: currently no ill effects - maybe sockets (ports) remain
%tied up until they time out?).
%
%
%
%\pagebreak


